{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human Activity Recognition using LSTM-RNN",
      "provenance": [],
      "authorship_tag": "ABX9TyMcjeVXXV3xaM36SaDbRieJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purnimapatel/Human-Activity-recognition/blob/master/Human_Activity_Recognition_using_LSTM_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZBDx-OdXjtE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "75003ebe-c54f-40b3-e00b-8c05ca8f2229"
      },
      "source": [
        "!pip install tensorflow==1.0.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/58/b71480f9ec9d08d581d672a81b15ab5fec36a5fcda2093558a23614d8468/tensorflow-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (44.5MB)\n",
            "\u001b[K     |████████████████████████████████| 44.5MB 84kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (0.35.1)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow==1.0.0) (50.3.0)\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed tensorflow-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxyCUEa3Yu82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95fbf965-a3ce-47f5-8b59-879181e8e51a"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name=\"UCI HAR Dataset (1).zip\"\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print(\"DONE\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BoS0oZ2b2TH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
        "from sklearn import metrics"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY5RSwtacEcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#those are separate normalised input features for the neural network\n",
        "INPUT_SIGNAL_TYPES = [\n",
        "    \"body_acc_x_\",\n",
        "    \"body_acc_y_\",\n",
        "    \"body_acc_z_\",\n",
        "    \"body_gyro_x_\",\n",
        "    \"body_gyro_y_\",\n",
        "    \"body_gyro_z_\",\n",
        "    \"total_acc_x_\",\n",
        "    \"total_acc_y_\",\n",
        "    \"total_acc_z_\"\n",
        "]\n",
        "\n",
        "# Output classes to learn how to classify\n",
        "LABELS = [\n",
        "    \"WALKING\",\n",
        "    \"WALKING_UPSTAIRS\",\n",
        "    \"WALKING_DOWNSTAIRS\",\n",
        "    \"SITTING\",\n",
        "    \"STANDING\",\n",
        "    \"LAYING\"\n",
        "]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQLgLXEVcUVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_X(X_signals_paths):\n",
        "    X_signals = []\n",
        "\n",
        "    for signal_type_path in X_signals_paths:\n",
        "        file = open(signal_type_path, 'r')\n",
        "        # Read dataset from disk, dealing with text files' syntax\n",
        "        X_signals.append(\n",
        "            [np.array(serie, dtype=np.float32) for serie in [\n",
        "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "            ]]\n",
        "        )\n",
        "        file.close()\n",
        "\n",
        "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
        "\n",
        "x_train_signals_paths = [\n",
        "    \"/content/UCI HAR Dataset/train/X_train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "x_test_signals_paths = [\n",
        "    \"/content/UCI HAR Dataset/test/X_test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "\n",
        "x_train = load_X(x_train_signals_paths)\n",
        "x_test = load_X(x_test_signals_paths)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kuNA_8ccjK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_y(y_path):\n",
        "    file = open(y_path, 'r')\n",
        "    # Read dataset from disk, dealing with text file's syntax\n",
        "    y_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "        ]],\n",
        "        dtype=np.int32\n",
        "    )\n",
        "    file.close()\n",
        "\n",
        "    # Substract 1 to each output class for friendly 0-based indexing\n",
        "    return y_ - 1\n",
        "\n",
        "y_train_path = \"/content/UCI HAR Dataset/train/y_train.txt\"\n",
        "y_test_path = \"/content/UCI HAR Dataset/test/y_test.txt\"\n",
        "\n",
        "y_train = load_y(y_train_path)\n",
        "y_test = load_y(y_test_path)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6srYs0dcopR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "312c5274-ac6b-45fc-aee7-22723797378f"
      },
      "source": [
        "print(x_test)\n",
        "print(x_train)\n",
        "print(y_test)\n",
        "print(y_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 0.25717777  0.25717777  0.25717777 ...  0.25717777  0.25717777\n",
            "    0.25717777]\n",
            "  [-0.02328523 -0.02328523 -0.02328523 ... -0.02328523 -0.02328523\n",
            "   -0.02328523]\n",
            "  [-0.01465376 -0.01465376 -0.01465376 ... -0.01465376 -0.01465376\n",
            "   -0.01465376]\n",
            "  ...\n",
            "  [-0.72000927 -0.72000927 -0.72000927 ... -0.72000927 -0.72000927\n",
            "   -0.72000927]\n",
            "  [ 0.27680105  0.27680105  0.27680105 ...  0.27680105  0.27680105\n",
            "    0.27680105]\n",
            "  [-0.0579783  -0.0579783  -0.0579783  ... -0.0579783  -0.0579783\n",
            "   -0.0579783 ]]\n",
            "\n",
            " [[ 0.28602672  0.28602672  0.28602672 ...  0.28602672  0.28602672\n",
            "    0.28602672]\n",
            "  [-0.01316336 -0.01316336 -0.01316336 ... -0.01316336 -0.01316336\n",
            "   -0.01316336]\n",
            "  [-0.11908252 -0.11908252 -0.11908252 ... -0.11908252 -0.11908252\n",
            "   -0.11908252]\n",
            "  ...\n",
            "  [-0.6980908  -0.6980908  -0.6980908  ... -0.6980908  -0.6980908\n",
            "   -0.6980908 ]\n",
            "  [ 0.28134292  0.28134292  0.28134292 ...  0.28134292  0.28134292\n",
            "    0.28134292]\n",
            "  [-0.08389802 -0.08389802 -0.08389802 ... -0.08389802 -0.08389802\n",
            "   -0.08389802]]\n",
            "\n",
            " [[ 0.27548483  0.27548483  0.27548483 ...  0.27548483  0.27548483\n",
            "    0.27548483]\n",
            "  [-0.02605042 -0.02605042 -0.02605042 ... -0.02605042 -0.02605042\n",
            "   -0.02605042]\n",
            "  [-0.11815167 -0.11815167 -0.11815167 ... -0.11815167 -0.11815167\n",
            "   -0.11815167]\n",
            "  ...\n",
            "  [-0.7027715  -0.7027715  -0.7027715  ... -0.7027715  -0.7027715\n",
            "   -0.7027715 ]\n",
            "  [ 0.28008303  0.28008303  0.28008303 ...  0.28008303  0.28008303\n",
            "    0.28008303]\n",
            "  [-0.07934619 -0.07934619 -0.07934619 ... -0.07934619 -0.07934619\n",
            "   -0.07934619]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.34996608  0.34996608  0.34996608 ...  0.34996608  0.34996608\n",
            "    0.34996608]\n",
            "  [ 0.03007744  0.03007744  0.03007744 ...  0.03007744  0.03007744\n",
            "    0.03007744]\n",
            "  [-0.11578796 -0.11578796 -0.11578796 ... -0.11578796 -0.11578796\n",
            "   -0.11578796]\n",
            "  ...\n",
            "  [-0.6553568  -0.6553568  -0.6553568  ... -0.6553568  -0.6553568\n",
            "   -0.6553568 ]\n",
            "  [ 0.2744788   0.2744788   0.2744788  ...  0.2744788   0.2744788\n",
            "    0.2744788 ]\n",
            "  [ 0.18118355  0.18118355  0.18118355 ...  0.18118355  0.18118355\n",
            "    0.18118355]]\n",
            "\n",
            " [[ 0.23759383  0.23759383  0.23759383 ...  0.23759383  0.23759383\n",
            "    0.23759383]\n",
            "  [ 0.01846687  0.01846687  0.01846687 ...  0.01846687  0.01846687\n",
            "    0.01846687]\n",
            "  [-0.09649893 -0.09649893 -0.09649893 ... -0.09649893 -0.09649893\n",
            "   -0.09649893]\n",
            "  ...\n",
            "  [-0.6597186  -0.6597186  -0.6597186  ... -0.6597186  -0.6597186\n",
            "   -0.6597186 ]\n",
            "  [ 0.26478162  0.26478162  0.26478162 ...  0.26478162  0.26478162\n",
            "    0.26478162]\n",
            "  [ 0.18756291  0.18756291  0.18756291 ...  0.18756291  0.18756291\n",
            "    0.18756291]]\n",
            "\n",
            " [[ 0.15362719  0.15362719  0.15362719 ...  0.15362719  0.15362719\n",
            "    0.15362719]\n",
            "  [-0.01843651 -0.01843651 -0.01843651 ... -0.01843651 -0.01843651\n",
            "   -0.01843651]\n",
            "  [-0.13701846 -0.13701846 -0.13701846 ... -0.13701846 -0.13701846\n",
            "   -0.13701846]\n",
            "  ...\n",
            "  [-0.66008025 -0.66008025 -0.66008025 ... -0.66008025 -0.66008025\n",
            "   -0.66008025]\n",
            "  [ 0.2639362   0.2639362   0.2639362  ...  0.2639362   0.2639362\n",
            "    0.2639362 ]\n",
            "  [ 0.1881034   0.1881034   0.1881034  ...  0.1881034   0.1881034\n",
            "    0.1881034 ]]]\n",
            "[[[ 0.2885845   0.2885845   0.2885845  ...  0.2885845   0.2885845\n",
            "    0.2885845 ]\n",
            "  [-0.02029417 -0.02029417 -0.02029417 ... -0.02029417 -0.02029417\n",
            "   -0.02029417]\n",
            "  [-0.13290514 -0.13290514 -0.13290514 ... -0.13290514 -0.13290514\n",
            "   -0.13290514]\n",
            "  ...\n",
            "  [-0.8412468  -0.8412468  -0.8412468  ... -0.8412468  -0.8412468\n",
            "   -0.8412468 ]\n",
            "  [ 0.17994061  0.17994061  0.17994061 ...  0.17994061  0.17994061\n",
            "    0.17994061]\n",
            "  [-0.05862692 -0.05862692 -0.05862692 ... -0.05862692 -0.05862692\n",
            "   -0.05862692]]\n",
            "\n",
            " [[ 0.27841884  0.27841884  0.27841884 ...  0.27841884  0.27841884\n",
            "    0.27841884]\n",
            "  [-0.01641057 -0.01641057 -0.01641057 ... -0.01641057 -0.01641057\n",
            "   -0.01641057]\n",
            "  [-0.12352019 -0.12352019 -0.12352019 ... -0.12352019 -0.12352019\n",
            "   -0.12352019]\n",
            "  ...\n",
            "  [-0.8447876  -0.8447876  -0.8447876  ... -0.8447876  -0.8447876\n",
            "   -0.8447876 ]\n",
            "  [ 0.1802889   0.1802889   0.1802889  ...  0.1802889   0.1802889\n",
            "    0.1802889 ]\n",
            "  [-0.05431672 -0.05431672 -0.05431672 ... -0.05431672 -0.05431672\n",
            "   -0.05431672]]\n",
            "\n",
            " [[ 0.27965307  0.27965307  0.27965307 ...  0.27965307  0.27965307\n",
            "    0.27965307]\n",
            "  [-0.01946716 -0.01946716 -0.01946716 ... -0.01946716 -0.01946716\n",
            "   -0.01946716]\n",
            "  [-0.11346169 -0.11346169 -0.11346169 ... -0.11346169 -0.11346169\n",
            "   -0.11346169]\n",
            "  ...\n",
            "  [-0.84893346 -0.84893346 -0.84893346 ... -0.84893346 -0.84893346\n",
            "   -0.84893346]\n",
            "  [ 0.18063731  0.18063731  0.18063731 ...  0.18063731  0.18063731\n",
            "    0.18063731]\n",
            "  [-0.04911781 -0.04911781 -0.04911781 ... -0.04911781 -0.04911781\n",
            "   -0.04911781]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.27338737  0.27338737  0.27338737 ...  0.27338737  0.27338737\n",
            "    0.27338737]\n",
            "  [-0.01701062 -0.01701062 -0.01701062 ... -0.01701062 -0.01701062\n",
            "   -0.01701062]\n",
            "  [-0.04502183 -0.04502183 -0.04502183 ... -0.04502183 -0.04502183\n",
            "   -0.04502183]\n",
            "  ...\n",
            "  [-0.7791326  -0.7791326  -0.7791326  ... -0.7791326  -0.7791326\n",
            "   -0.7791326 ]\n",
            "  [ 0.24914484  0.24914484  0.24914484 ...  0.24914484  0.24914484\n",
            "    0.24914484]\n",
            "  [ 0.04081119  0.04081119  0.04081119 ...  0.04081119  0.04081119\n",
            "    0.04081119]]\n",
            "\n",
            " [[ 0.28965417  0.28965417  0.28965417 ...  0.28965417  0.28965417\n",
            "    0.28965417]\n",
            "  [-0.01884304 -0.01884304 -0.01884304 ... -0.01884304 -0.01884304\n",
            "   -0.01884304]\n",
            "  [-0.1582806  -0.1582806  -0.1582806  ... -0.1582806  -0.1582806\n",
            "   -0.1582806 ]\n",
            "  ...\n",
            "  [-0.7851814  -0.7851814  -0.7851814  ... -0.7851814  -0.7851814\n",
            "   -0.7851814 ]\n",
            "  [ 0.24643223  0.24643223  0.24643223 ...  0.24643223  0.24643223\n",
            "    0.24643223]\n",
            "  [ 0.02533948  0.02533948  0.02533948 ...  0.02533948  0.02533948\n",
            "    0.02533948]]\n",
            "\n",
            " [[ 0.35150346  0.35150346  0.35150346 ...  0.35150346  0.35150346\n",
            "    0.35150346]\n",
            "  [-0.01242312 -0.01242312 -0.01242312 ... -0.01242312 -0.01242312\n",
            "   -0.01242312]\n",
            "  [-0.20386717 -0.20386717 -0.20386717 ... -0.20386717 -0.20386717\n",
            "   -0.20386717]\n",
            "  ...\n",
            "  [-0.7832669  -0.7832669  -0.7832669  ... -0.7832669  -0.7832669\n",
            "   -0.7832669 ]\n",
            "  [ 0.24680851  0.24680851  0.24680851 ...  0.24680851  0.24680851\n",
            "    0.24680851]\n",
            "  [ 0.03669484  0.03669484  0.03669484 ...  0.03669484  0.03669484\n",
            "    0.03669484]]]\n",
            "[[4]\n",
            " [4]\n",
            " [4]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "[[4]\n",
            " [4]\n",
            " [4]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ-VKIUJc5SG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "46691bac-94c8-453b-f37e-11ad08052285"
      },
      "source": [
        "training_data_count = len(x_train)\n",
        "test_data_count = len(x_test) \n",
        "print(training_data_count)\n",
        "print(test_data_count)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7352\n",
            "2947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI-6JdsWc7xv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "039fde8e-c78a-4c3f-8cbb-06eec5321d28"
      },
      "source": [
        "n_steps=len(x_train[0])\n",
        "n_input=len(x_train[0][0])\n",
        "print(n_steps)\n",
        "print(n_input)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "561\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUtxAYpmdGZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#internal structure of the neural network \n",
        "n_hidden = 32 # Hidden layer num of features\n",
        "n_classes = 6"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDU0TQKfdV-k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "683d6c8b-9c58-46b0-e8ce-5f59104c5600"
      },
      "source": [
        " #Training\n",
        "\n",
        "learning_rate = 0.0025\n",
        "lambda_loss_amount = 0.0015\n",
        "training_iters = training_data_count * 300  # Loop 300 times on the dataset\n",
        "batch_size = 1500\n",
        "display_iter = 30000  # To show test set accuracy during training\n",
        "# Some debugging info\n",
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(x_test.shape, y_test.shape, np.mean(x_test), np.std(x_test))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(X shape, y shape, every X's mean, every X's standard deviation)\n",
            "(2947, 561, 9) (2947, 1) -0.50925714 0.525582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTIuBWT-dq-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LSTM_RNN(_X, _weights, _biases):\n",
        "    # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters.\n",
        "    # Moreover, two LSTM cells are stacked which adds deepness to the neural network.\n",
        "    # Note, some code of this notebook is inspired from an slightly different\n",
        "    # RNN architecture used on another dataset, some of the credits goes to\n",
        "    # \"aymericdamien\" under the MIT license.\n",
        "\n",
        "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
        "    # input shape: (batch_size, n_steps, n_input)\n",
        "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
        "    # Reshape to prepare input to hidden activation\n",
        "    _X = tf.reshape(_X, [-1, n_input])\n",
        "    # new shape: (n_steps*batch_size, n_input)\n",
        "\n",
        "    # ReLU activation, thanks to Yu Zhao for adding this improvement here:\n",
        "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
        "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
        "    _X = tf.split(_X, n_steps, 0)\n",
        "    # new shape: n_steps * (batch_size, n_hidden)\n",
        "\n",
        "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
        "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
        "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
        "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
        "    # Get LSTM cell output\n",
        "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
        "\n",
        "    # Get last time step's output feature for a \"many-to-one\" style classifier,\n",
        "    # as in the image describing RNNs at the top of this page\n",
        "    lstm_last_output = outputs[-1]\n",
        "\n",
        "    # Linear activation\n",
        "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
        "\n",
        "\n",
        "def extract_batch_size(_train, step, batch_size):\n",
        "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data.\n",
        "\n",
        "    shape = list(_train.shape)\n",
        "    shape[0] = batch_size\n",
        "    batch_s = np.empty(shape)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Loop index\n",
        "        index = ((step-1)*batch_size + i) % len(_train)\n",
        "        batch_s[i] = _train[index]\n",
        "\n",
        "    return batch_s\n",
        "\n",
        "\n",
        "def one_hot(y_, n_classes=n_classes):\n",
        "    # Function to encode neural one-hot output labels from number indexes\n",
        "    # e.g.:\n",
        "    # one_hot(y_=[[5], [0], [3]], n_classes=6):\n",
        "    #     return [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
        "\n",
        "    y_ = y_.reshape(len(y_))\n",
        "    return np.eye(n_classes)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD9iDrpYdyZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's build the neural network:\n",
        "# Graph input/output\n",
        "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "# Graph weights\n",
        "weights = {\n",
        "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
        "}\n",
        "biases = {\n",
        "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "pred = LSTM_RNN(x, weights, biases)\n",
        "\n",
        "# Loss, optimizer and evaluation\n",
        "l2 = lambda_loss_amount * sum(\n",
        "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
        ") # L2 loss prevents this overkill neural network to overfit the data\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSTT94hKeRh0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d31ed350-5747-40a1-e78e-d79383eafdfa"
      },
      "source": [
        "# To keep track of training's performance\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "# Launch the graph\n",
        "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "# Perform Training steps with \"batch_size\" amount of example data at each loop\n",
        "step = 1\n",
        "while step * batch_size <= training_iters:\n",
        "    batch_xs =         extract_batch_size(x_train, step, batch_size)\n",
        "    batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
        "\n",
        "    # Fit training using batch data\n",
        "    _, loss, acc = sess.run(\n",
        "        [optimizer, cost, accuracy],\n",
        "        feed_dict={\n",
        "            x: batch_xs,\n",
        "            y: batch_ys\n",
        "        }\n",
        "    )\n",
        "    train_losses.append(loss)\n",
        "    train_accuracies.append(acc)\n",
        "\n",
        "    # Evaluate network only at some steps for faster training:\n",
        "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
        "\n",
        "        # To not spam console, show training accuracy/loss in this \"if\"\n",
        "        print(\"Training iter #\" + str(step*batch_size) + \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \", Accuracy = {}\".format(acc))\n",
        "\n",
        "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
        "        loss, acc = sess.run(\n",
        "            [cost, accuracy],\n",
        "            feed_dict={\n",
        "                x: x_test,\n",
        "                y: one_hot(y_test)\n",
        "            }\n",
        "        )\n",
        "        test_losses.append(loss)\n",
        "        test_accuracies.append(acc)\n",
        "        print(\"PERFORMANCE ON TEST SET: \" + \"Batch Loss = {}\".format(loss) + \", Accuracy = {}\".format(acc))\n",
        "\n",
        "    step += 1\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "\n",
        "# Accuracy for test data\n",
        "\n",
        "one_hot_predictions, accuracy, final_loss = sess.run(\n",
        "    [pred, accuracy, cost],\n",
        "    feed_dict={\n",
        "        x: x_test,\n",
        "        y: one_hot(y_test)\n",
        "    }\n",
        ")\n",
        "test_losses.append(final_loss)\n",
        "test_accuracies.append(accuracy)\n",
        "\n",
        "print(\"FINAL RESULT: \" + \"Batch Loss = {}\".format(final_loss) + \", Accuracy = {}\".format(accuracy))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training iter #1500:   Batch Loss = 3.127519, Accuracy = 0.20666666328907013\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 2.723475694656372, Accuracy = 0.27010518312454224\n",
            "Training iter #30000:   Batch Loss = 1.787492, Accuracy = 0.5460000038146973\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 1.776181936264038, Accuracy = 0.5188326835632324\n",
            "Training iter #60000:   Batch Loss = 1.548670, Accuracy = 0.6639999747276306\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 1.5227867364883423, Accuracy = 0.6392942070960999\n",
            "Training iter #90000:   Batch Loss = 1.350784, Accuracy = 0.718666672706604\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 1.335038423538208, Accuracy = 0.7139464020729065\n",
            "Training iter #120000:   Batch Loss = 1.180631, Accuracy = 0.7699999809265137\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 1.221150279045105, Accuracy = 0.7543264627456665\n",
            "Training iter #150000:   Batch Loss = 1.122570, Accuracy = 0.7820000052452087\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 1.1719768047332764, Accuracy = 0.7831693291664124\n",
            "Training iter #180000:   Batch Loss = 1.069328, Accuracy = 0.7879999876022339\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 1.1170458793640137, Accuracy = 0.8014930486679077\n",
            "Training iter #210000:   Batch Loss = 0.983635, Accuracy = 0.8206666707992554\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 1.1007444858551025, Accuracy = 0.798439085483551\n",
            "Training iter #240000:   Batch Loss = 0.855640, Accuracy = 0.878000020980835\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 1.0589959621429443, Accuracy = 0.8215134143829346\n",
            "Training iter #270000:   Batch Loss = 0.838810, Accuracy = 0.8980000019073486\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 1.0925049781799316, Accuracy = 0.8038683533668518\n",
            "Training iter #300000:   Batch Loss = 0.854092, Accuracy = 0.8920000195503235\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 1.027316927909851, Accuracy = 0.8201560974121094\n",
            "Training iter #330000:   Batch Loss = 0.917862, Accuracy = 0.8846666812896729\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 1.0109343528747559, Accuracy = 0.8323718905448914\n",
            "Training iter #360000:   Batch Loss = 0.810470, Accuracy = 0.9020000100135803\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.9681350588798523, Accuracy = 0.8367831707000732\n",
            "Training iter #390000:   Batch Loss = 0.864241, Accuracy = 0.878000020980835\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.9481563568115234, Accuracy = 0.8449270725250244\n",
            "Training iter #420000:   Batch Loss = 0.821514, Accuracy = 0.8846666812896729\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.933660626411438, Accuracy = 0.8449270725250244\n",
            "Training iter #450000:   Batch Loss = 0.780118, Accuracy = 0.8920000195503235\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.9430767893791199, Accuracy = 0.8445877432823181\n",
            "Training iter #480000:   Batch Loss = 0.774367, Accuracy = 0.8920000195503235\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.9204767942428589, Accuracy = 0.8483203053474426\n",
            "Training iter #510000:   Batch Loss = 0.784175, Accuracy = 0.8846666812896729\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.9201265573501587, Accuracy = 0.8517135977745056\n",
            "Training iter #540000:   Batch Loss = 0.843001, Accuracy = 0.8586666584014893\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.9214920997619629, Accuracy = 0.84764164686203\n",
            "Training iter #570000:   Batch Loss = 0.720539, Accuracy = 0.9020000100135803\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.8833836317062378, Accuracy = 0.8595181703567505\n",
            "Training iter #600000:   Batch Loss = 0.657148, Accuracy = 0.9253333210945129\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.8716732859611511, Accuracy = 0.8551068902015686\n",
            "Training iter #630000:   Batch Loss = 0.594038, Accuracy = 0.9593333601951599\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.8664647936820984, Accuracy = 0.857821524143219\n",
            "Training iter #660000:   Batch Loss = 0.614495, Accuracy = 0.95333331823349\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.8559629917144775, Accuracy = 0.8585001826286316\n",
            "Training iter #690000:   Batch Loss = 0.699754, Accuracy = 0.9126666784286499\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.8949693441390991, Accuracy = 0.8449270725250244\n",
            "Training iter #720000:   Batch Loss = 0.653738, Accuracy = 0.9340000152587891\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.8313198089599609, Accuracy = 0.8669833540916443\n",
            "Training iter #750000:   Batch Loss = 0.648765, Accuracy = 0.9326666593551636\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.8331347703933716, Accuracy = 0.8669833540916443\n",
            "Training iter #780000:   Batch Loss = 0.624843, Accuracy = 0.9293333292007446\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.813029944896698, Accuracy = 0.8703766465187073\n",
            "Training iter #810000:   Batch Loss = 0.615055, Accuracy = 0.9300000071525574\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.8080990314483643, Accuracy = 0.8686800003051758\n",
            "Training iter #840000:   Batch Loss = 0.640001, Accuracy = 0.9126666784286499\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.8094204664230347, Accuracy = 0.8642687201499939\n",
            "Training iter #870000:   Batch Loss = 0.622845, Accuracy = 0.9240000247955322\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7996878623962402, Accuracy = 0.870037317276001\n",
            "Training iter #900000:   Batch Loss = 0.679641, Accuracy = 0.8953333497047424\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.8117451667785645, Accuracy = 0.8585001826286316\n",
            "Training iter #930000:   Batch Loss = 0.620611, Accuracy = 0.9226666688919067\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7966880202293396, Accuracy = 0.8696979880332947\n",
            "Training iter #960000:   Batch Loss = 0.567284, Accuracy = 0.9359999895095825\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7818418145179749, Accuracy = 0.8720732927322388\n",
            "Training iter #990000:   Batch Loss = 0.499348, Accuracy = 0.9639999866485596\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.868985652923584, Accuracy = 0.8347471952438354\n",
            "Training iter #1020000:   Batch Loss = 0.518474, Accuracy = 0.9586666822433472\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7843490839004517, Accuracy = 0.8642687201499939\n",
            "Training iter #1050000:   Batch Loss = 0.512368, Accuracy = 0.9586666822433472\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7535529732704163, Accuracy = 0.8751272559165955\n",
            "Training iter #1080000:   Batch Loss = 0.541203, Accuracy = 0.9573333263397217\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7328730821609497, Accuracy = 0.8761452436447144\n",
            "Training iter #1110000:   Batch Loss = 0.571331, Accuracy = 0.9333333373069763\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.772251307964325, Accuracy = 0.8642687201499939\n",
            "Training iter #1140000:   Batch Loss = 0.554081, Accuracy = 0.9419999718666077\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7453517317771912, Accuracy = 0.8629114627838135\n",
            "Training iter #1170000:   Batch Loss = 0.510107, Accuracy = 0.9433333277702332\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7197855710983276, Accuracy = 0.8761452436447144\n",
            "Training iter #1200000:   Batch Loss = 0.508920, Accuracy = 0.9446666836738586\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.727818489074707, Accuracy = 0.8727519512176514\n",
            "Training iter #1230000:   Batch Loss = 0.528014, Accuracy = 0.937333345413208\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7242509126663208, Accuracy = 0.8686800003051758\n",
            "Training iter #1260000:   Batch Loss = 0.493678, Accuracy = 0.9553333520889282\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7375061511993408, Accuracy = 0.8696979880332947\n",
            "Training iter #1290000:   Batch Loss = 0.503113, Accuracy = 0.9386666417121887\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7151929140090942, Accuracy = 0.8761452436447144\n",
            "Training iter #1320000:   Batch Loss = 0.492045, Accuracy = 0.9353333115577698\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7133955955505371, Accuracy = 0.8747879266738892\n",
            "Training iter #1350000:   Batch Loss = 0.475528, Accuracy = 0.9380000233650208\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.9298619627952576, Accuracy = 0.8143875002861023\n",
            "Training iter #1380000:   Batch Loss = 0.539037, Accuracy = 0.9293333292007446\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7330527305603027, Accuracy = 0.8591788411140442\n",
            "Training iter #1410000:   Batch Loss = 0.435757, Accuracy = 0.9693333506584167\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7129150629043579, Accuracy = 0.8724126219749451\n",
            "Training iter #1440000:   Batch Loss = 0.457940, Accuracy = 0.9633333086967468\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6918307542800903, Accuracy = 0.8758059144020081\n",
            "Training iter #1470000:   Batch Loss = 0.467683, Accuracy = 0.9593333601951599\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7156291007995605, Accuracy = 0.8727519512176514\n",
            "Training iter #1500000:   Batch Loss = 0.493256, Accuracy = 0.9399999976158142\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.7161182761192322, Accuracy = 0.8663046956062317\n",
            "Training iter #1530000:   Batch Loss = 0.456416, Accuracy = 0.9486666917800903\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6990596055984497, Accuracy = 0.8710553050041199\n",
            "Training iter #1560000:   Batch Loss = 0.455044, Accuracy = 0.9506666660308838\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6643537282943726, Accuracy = 0.8720732927322388\n",
            "Training iter #1590000:   Batch Loss = 0.474607, Accuracy = 0.9393333196640015\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6704257726669312, Accuracy = 0.876823902130127\n",
            "Training iter #1620000:   Batch Loss = 0.431885, Accuracy = 0.9626666903495789\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6790752410888672, Accuracy = 0.873430609703064\n",
            "Training iter #1650000:   Batch Loss = 0.467028, Accuracy = 0.9340000152587891\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6985422968864441, Accuracy = 0.8622328042984009\n",
            "Training iter #1680000:   Batch Loss = 0.460782, Accuracy = 0.9346666932106018\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6801143884658813, Accuracy = 0.8635901212692261\n",
            "Training iter #1710000:   Batch Loss = 0.462747, Accuracy = 0.9259999990463257\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6702854633331299, Accuracy = 0.8771632313728333\n",
            "Training iter #1740000:   Batch Loss = 0.417639, Accuracy = 0.9599999785423279\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.646857738494873, Accuracy = 0.8724126219749451\n",
            "Training iter #1770000:   Batch Loss = 0.439709, Accuracy = 0.9486666917800903\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6455439329147339, Accuracy = 0.8761452436447144\n",
            "Training iter #1800000:   Batch Loss = 0.397815, Accuracy = 0.9666666388511658\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6291713118553162, Accuracy = 0.8788598775863647\n",
            "Training iter #1830000:   Batch Loss = 0.438075, Accuracy = 0.9526666402816772\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6283023357391357, Accuracy = 0.882253110408783\n",
            "Training iter #1860000:   Batch Loss = 0.429118, Accuracy = 0.9513333439826965\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6336153745651245, Accuracy = 0.8808958530426025\n",
            "Training iter #1890000:   Batch Loss = 0.395265, Accuracy = 0.9620000123977661\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6245748996734619, Accuracy = 0.885646402835846\n",
            "Training iter #1920000:   Batch Loss = 0.388560, Accuracy = 0.9620000123977661\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.626308262348175, Accuracy = 0.8839497566223145\n",
            "Training iter #1950000:   Batch Loss = 0.401006, Accuracy = 0.9573333263397217\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6453194618225098, Accuracy = 0.8737699389457703\n",
            "Training iter #1980000:   Batch Loss = 0.408990, Accuracy = 0.9539999961853027\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6516801714897156, Accuracy = 0.8646080493927002\n",
            "Training iter #2010000:   Batch Loss = 0.428065, Accuracy = 0.949999988079071\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6336883902549744, Accuracy = 0.8737699389457703\n",
            "Training iter #2040000:   Batch Loss = 0.420295, Accuracy = 0.9399999976158142\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.5934706926345825, Accuracy = 0.8832710981369019\n",
            "Training iter #2070000:   Batch Loss = 0.397160, Accuracy = 0.9520000219345093\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6644139289855957, Accuracy = 0.8659653663635254\n",
            "Training iter #2100000:   Batch Loss = 0.345696, Accuracy = 0.972000002861023\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6022623777389526, Accuracy = 0.8825924396514893\n",
            "Training iter #2130000:   Batch Loss = 0.350510, Accuracy = 0.9713333249092102\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6143612861633301, Accuracy = 0.8751272559165955\n",
            "Training iter #2160000:   Batch Loss = 0.368327, Accuracy = 0.9620000123977661\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6262484192848206, Accuracy = 0.8781812191009521\n",
            "Training iter #2190000:   Batch Loss = 0.376630, Accuracy = 0.9639999866485596\n",
            "PERFORMANCE ON TEST SET: Batch Loss = 0.6327196955680847, Accuracy = 0.8696979880332947\n",
            "Optimization Finished!\n",
            "FINAL RESULT: Batch Loss = 0.6446419954299927, Accuracy = 0.8663046956062317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elu4-hUlcvWZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54972317-48a3-4e62-8d2e-d8de2fae44c5"
      },
      "source": [
        "#visualiZation of the result\n",
        "predictions = one_hot_predictions.argmax(1)\n",
        "\n",
        "print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
        "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
        "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Confusion Matrix:\")\n",
        "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
        "print(confusion_matrix)\n",
        "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
        "\n",
        "print(\"\")\n",
        "print(\"Confusion matrix (normalised to % of total test data):\")\n",
        "print(normalised_confusion_matrix)\n",
        "print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
        "print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
        "\n",
        "# Plot Results:\n",
        "width = 12\n",
        "height = 12\n",
        "plt.figure(figsize=(width, height))\n",
        "plt.imshow(\n",
        "    normalised_confusion_matrix,\n",
        "    interpolation='nearest',\n",
        "    cmap=plt.cm.rainbow\n",
        ")\n",
        "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(n_classes)\n",
        "plt.xticks(tick_marks, LABELS, rotation=90)\n",
        "plt.yticks(tick_marks, LABELS)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy: 86.63046956062317%\n",
            "\n",
            "Precision: 86.98466418351313%\n",
            "Recall: 86.63047166610112%\n",
            "f1_score: 86.50732876802357%\n",
            "\n",
            "Confusion Matrix:\n",
            "[[489   3   4   0   0   0]\n",
            " [ 87 358  26   0   0   0]\n",
            " [ 34  60 326   0   0   0]\n",
            " [  0   3   0 389  94   5]\n",
            " [  0   2   0  68 462   0]\n",
            " [  0   2   0   0   6 529]]\n",
            "\n",
            "Confusion matrix (normalised to % of total test data):\n",
            "[[16.593145    0.10179844  0.13573125  0.          0.          0.        ]\n",
            " [ 2.9521546  12.147947    0.88225317  0.          0.          0.        ]\n",
            " [ 1.1537156   2.0359688  11.062097    0.          0.          0.        ]\n",
            " [ 0.          0.10179844  0.         13.199864    3.1896844   0.16966406]\n",
            " [ 0.          0.06786563  0.          2.3074312  15.67696     0.        ]\n",
            " [ 0.          0.06786563  0.          0.          0.20359688 17.950459  ]]\n",
            "Note: training and testing data is not equally distributed amongst classes, \n",
            "so it is normal that more than a 6th of the data is correctly classifier in the last category.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzUAAAM8CAYAAABu444kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZglZXn38e9vWEUQEFCBiINrFBCCkygYFUUjKsQlLkHMK8a8ajRxAdQYTCQmom+UgGsMRkVFCRiXoLhgooTIJgMMmwhuqOybIFtYpu/3j6oOx04vp2em5/TD+X6u61xzquqpp+6qbrTufu56KlWFJEmSJLVqyagDkCRJkqTVYVIjSZIkqWkmNZIkSZKaZlIjSZIkqWkmNZIkSZKaZlIjSZIkqWnrjjoASZIkSatur6SuG3UQMzgLvllVey30cUxqJEmSpIZdBywfdRAzCGy5No5j+ZkkSZKkpjlSI0mSJLVunUU6VrFyYq0cZpGevSRJkiQNx6RGkiRJUtMsP5MkSZJaFmCdjDqK6a1cO4dxpEaSJElS00xqJEmSJDXN8jNJkiSpaVm8s5+tpfqzxXr2kiRJkjQUkxpJkiRJTTOpkSRJktQ0n6mRJEmSWhZg3UU6pfNa4kiNJEmSpKaZ1EiSJElqmuVnkiRJUsvCIp7See0Y77OXJEmS1DyTGkmSJElNs/xMkiRJat06zn4mSZIkSc0yqZEkSZLUNMvPJEmSpJYlzn426gAkSZIkaXWY1EiSJElqmuVnkiRJUst8+aYjNZIkSZLaZlIjSZIkqWmWn0mSJEmt8+WbkiRJktQukxpJkiRJTbP8TJIkSWqZL990pEaSJElS20xqJEmSJDXN8jNJkiSpdc5+JkmSJEntMqmRJEmS1DTLzyRJkqSWBVh3vMcqxvvsJUmSJDXPpEaSJElS0yw/kyRJklqWOPvZqAOQJEmSpNVhUiNJkiSpaZafSZIkSa1bZ7zHKsb77CVJkiQ1z6RGkiRJUtMsP5MkSZJaFiw/G3UAkiRJkrQ6TGokSZIkNc3yM0mSJKllvnzTkRpJkiRJbTOpkSRJktQ0y88kSZKk1jn7mSRJkiS1y6RGkiRJUtMsP5MkSZJaFpz9bNQBSJIkSdLqMKmRJEmS1DTLzyRJkqSmxdnPRh2AJEmSJK0OkxpJkiRJTbP8TJIkSWqZs585UiNJkiSpbSY1kiRJkppm+ZkkSZLUsuDsZ6MOQJIkSZJWh0mNJEmSpKZZfiZJkiS1ztnPJEmSJKldJjWSJEmSmmZSI0laI5LcJ8lXktyU5POr0c9+SU5ck7GNSpInJbl41HFIupdLutnPFuNnLTGpkaQxk+SlSZYnuSXJlUm+nuR310DXLwQeCGxRVS9a1U6q6rNV9XtrIJ4FlaSSPHy2NlX1X1X1qLUVkySNK5MaSRojSQ4AjgAOpUtAtgM+Ajx3DXT/EOCSqrp7DfTVvCROxiNJa4lJjSSNiSSbAu8EXldVX6yqW6vqrqr6SlW9uW+zQZIjklzRf45IskG/bY8klyU5MMk1/SjPK/ptfwP8NfCSfgTolUkOSXL0wPGX9qMb6/bL+yf5SZKbk/w0yX4D6787sN/uSc7sy9rOTLL7wLaTkvxtklP6fk5MsuUM5z8Z/1sG4n9ekmcnuSTJDUn+cqD97yQ5LcmNfdsPJVm/33Zy3+zc/nxfMtD/W5NcBXxycl2/z8P6Y+zaL2+T5Noke6zWD1aSYPRlZpafSZLWkt2ADYEvzdLmYOAJwC7AzsDvAG8f2P4gYFNgW+CVwIeTbF5V76Ab/Tm2qjauqo/PFkiS+wIfAJ5VVZsAuwMrpml3f+CEvu0WwD8AJyTZYqDZS4FXAA8A1gcOmuXQD6K7BtvSJWEfA14GPA54EvBXSbbv264E3gRsSXft9gReC1BVT+7b7Nyf77ED/d+fbtTqVYMHrqofA28Fjk6yEfBJ4FNVddIs8UqShmBSI0njYwvgujnKw/YD3llV11TVtcDfAH80sP2ufvtdVfU14BZgVZ8ZmQB2THKfqrqyqi6cps1zgB9W1Weq6u6qOgb4AbDPQJtPVtUlVXU7cBxdQjaTu4B3VdVdwL/QJSzvr6qb++N/ny6Zo6rOqqrT++NeCvwT8JQhzukdVXVHH8+vqaqPAT8CzgC2pksiJUmryaRGksbH9cCWczzrsQ3ws4Hln/Xr/qePKUnRbcDG8w2kqm4FXgK8BrgyyQlJfnOIeCZj2nZg+ap5xHN9Va3sv08mHVcPbL99cv8kj0zy1SRXJfkV3UjUtKVtA66tqv+eo83HgB2BD1bVHXO0laS5he7lm4vxs5aY1EjS+DgNuAN43ixtrqArnZq0Xb9uVdwKbDSw/KDBjVX1zap6Bt2IxQ/obvbnimcypstXMab5+Ee6uB5RVfcD/pLu1mE2NdvGJBvTTdTwceCQvrxOkrSaTGokaUxU1U10z5F8uH9AfqMk6yV5VpK/75sdA7w9yVb9A/d/DRw9U59zWAE8Ocl2/SQFb5vckOSBSZ7bP1tzB10Z28Q0fXwNeGQ/DfW6SV4CPAb46irGNB+bAL8CbulHkf50yvargYfOs8/3A8ur6k/onhX66GpHKUkyqZGkcVJVhwEH0D38fy3wC+DPgC/3Tf4OWA6cB5wPnN2vW5VjfQs4tu/rLH49EVnSx3EFcAPdsypTkwaq6npgb+BAuvK5twB7V9V1qxLTPB1ENwnBzXSjSMdO2X4I8Kl+drQXz9VZkucCe3HPeR4A7Do565skrTJfvkmqZh0plyRJkrSILdv8PrV8j+3nbjgC+fJFZ1XVshm3J5+g++PVNVW148D6PwdeRzcT5QlV9ZbZjuNIjSRJkqRROYpuFPt/JHkq3Uuhd66qHYD3zdWJbzuWJEmSWrcWZxpbk6rq5CRLp6z+U+A9kzNEVtU1c/XjSI0kSZKkxeSRwJOSnJHkP5P89lw7OFIjSZIkaaFsmWT5wPKRVXXkHPusC9wfeALw28BxSR5as0wG4EiNJM0iybuTvHHUccwlyf5JvjuwfEuS+U43PNcxjkqySjOhjVKSJyb5YX9NZntHz0Idf9FetySV5OEL1Pf3kuywEH1LmiKMfpazmWc/u66qlg185kpoAC4Dvlid79FN+T/ry49NaiRpBkm2Av4P8E+jjmW+qmrjqvrJ2jre1KRqFfbfM8lPk1yV5A8H1m+W5Owkm6xGeO8EPtRfky9P3Zjk0iRPn0es82o/R1+rdd2m9LXG4pqm76V9AjSfCo/30V17SZqvLwNPBUjySGB9YNap/E1qJGlm+wNfq6rb13TH87w5HAdHAPsAzwQ+kmSdfv276R4WvXk1+n4IcOFqxqf5Ox54apIHjToQSYtXkmOA04BHJbksySuBTwAPTXIB8C/Ay2crPQOTGkmazbOA/5xcSLJH/z+4Bya5JsmVSV4xsH3TJJ9Ocm2SnyV5e5Il/bb9k5yS5PAk1wOH9GVJH0ny9b406pQkD0pyRJJfJvlBkt8a6P8vkvw4yc1Jvp/k+TMFPlhWlOTZffubk1ye5KCBdnsnWdG/QPLUJI8d2PZb/SjJzUmOBTac4ViPBj4K7Nafx41zXY9p3LeqLqiqc4E7gS2S/A6wfVUdN+NP6J4Y/m+SHyW5IcnxSbbp1/8YeCjwlT62Dabs9xlgu4Htb+nX/36SC/vrclJ/jrO1/3w/ynRTkpOHKbua5bptkOR9SX6e5OokH01yn37blkm+2sd1Q5L/SrJkprimOeab+9/bK5L88ZRtz0lyTpJfJflFkkMGNp/c/3tj3/9uSR6W5NtJrk9yXZLPJtlscoeq+m+6l64+c65rIWk1Jd3sZ4vxM4eq2reqtq6q9arqN6rq41V1Z1W9rKp2rKpdq+rbc/VjUiNJM9sJuHjKugcBmwLbAq8EPpxk837bB/ttDwWeQle69oqBfR8P/AR4IPCuft2LgbfT1QrfQffXqrP75X8F/mFg/x8DT+qP8TfA0Um2HuI8Pg68uqo2AXYEvg1d0kL317BXA1vQldkd399Ur083/P8Zuoc1Pw/8wXSdV9VFwGuA0/oSr8kb27mux6BrkuycZGe62ulfAu8HXj/XySV5Gt2IzouBrYGf0f1lj6p6GPBzYJ8+tjumxP5HU7b/fbpSh2OANwJbAV+jSxbWn65939XXgUcAD6D7+X12rrhnuW7voZv5Zxfg4XS/a3/dbzuQrtZ8K7rfo7/supoxrsHrtBdwEPCMPtappWq30v2MNgOeA/xp7nkG6cn9v5v1/Z9GV8X/bmAb4NHAg4FDpvR5EbDzXNdCklaXSY0kzWwzYGrZ013AO6vqrqr6GnAL3ZD5OsAfAm+rqpur6lLgMOCPBva9oqo+WFV3D5S0famqzur/qv0l4L+r6tNVtRI4FvifkZqq+nxVXVFVE1V1LPBD4HeGOI+7gMckuV9V/bKqzu7Xvwr4p6o6o6pWVtWn6BKrJ/Sf9YAj+nP9V+DMYS4awJDXY9Br6JKYI/s2fwr8O7Bhkm8m+U6Sp8yw737AJ6rq7D5peRvd6MfSYeOd4iV0b6/+VlXdRfdsyH2A3Wfaoao+0Z/nHXQ39jsn2XS+B04Sup/Lm6rqhr7s7lC6awndz3Jr4CH9z+W/5irJGPBi4JP9iNitTElAquqkqjq///06jy6xm+maU1U/6q/RHVV1LV0CPrX9zXT/HUnSgrKmW5Jm9ktg6gPq11fV3QPLtwEb042srEc3SjDpZ3R/ZZ/0i2mOcfXA99unWd54ciHJ/wEOAJb2qyaPO5c/oBsNek+S84C/6P/S/hDg5Un+fKDt+nR/eS/g8ik3zIPnNpdhrsf/qKoVwB4A/ejTYcBudOV/bwSuAE5O8pBpbuK3oRsdmezrlnQlftsCl84j5sH+/ifuqppI8ouZYu8TuHcBL6IbQZnoN20J3DTPY28FbASc1eU33SGAyWeM3kuXjJzYbz+yqt4zZN/b0JWDTfq1n2eSx9ONEu1I93uwAd0I3bSSPJAuEX0S3X8nS+j+mxm0CXDjkPFJWh3rjPdYxXifvSTN7jy6MqBhXEf3V/SHDKzbDrh8YHnYv6j/L0keAnwM+DNgi75U6QK6G95ZVdWZVfVcutKoLwOTz6j8AnhXVW028Nmoqo4BrgS2zcCddX8+Mx5myvIw12MmhwNv70ezdgKW9yM969Hd9E91xeBxktyXrpxumGNNF/vU/kJXWnX5DO1fCjyXrpxrU+5JOod5vfd01+12YIeBn8mmVbUxQD8adGBVPRT4feCAJHvO0NdUV/bnMWnqz/NzdA/3P7iqNqV73mfyHKbr+9B+/U5VdT/gZfzvc340cO4ccUnSajOpkaSZfY1Zym8G9eVixwHvSrJJn4QcABy9hmK5L90N5LUA6SYo2HGunZKsn2S/JJv2pVS/4p6RhI8Br0ny+HTu2z8svgndsz13A69Psl6SFzB7qdvVwG/0z+Ks8vVI8gxgw6r6ar/qp8DT+gfvNwCun2a3Y4BXJNmlnwjgUOCMPhEaxtV0z/1MOg54Trpppteje47lDuDUGdpv0m+/nm6U5dAhjzvZ1+B1m6D7uRye5AEASbZN8sz++95JHt4nWjcBK7nn5zk1rqmOA/ZP8pgkGwHvmLJ9E+CGqvrvdJM0vHRg27X9caae9y3ATUm2Bd482FmSDYHHAd8a4jpI0moxqZGkmX0aePbkzFND+HO6h61/AnyX7i/fn1gTgVTV9+lKsk6ju3ndCThlyN3/CLg0ya/onl3Zr+9zOfB/gQ/RlQ39iG4aa6rqTuAF/fINdM+ZfHGWY3ybbtrkq5JMvktgXtejT0jeC7xhYPWf040Y/Dvw2j5Z+jVV9e/AXwFfoBuNeBj3PIMyjHcDb+9nFDuoqi6mG3X4IN3IyT50D+DfOV17ut+Tn9GN5HwfOH0ex57uur2V7mdxev8z+3fgUf22R/TLt9D9Lnykqr4zQ1y/pqq+Tjd19rf7/qfOJvRa4J1JbqabmOC4gX1voyuxO6Xv/wl0k1XsSpdcncD//v3YBzipqq6Yx/WQtCrC6Gc5W8XZz9bYJRj++UJJGj9JDgWuqaojRh2L1JIkZwCvrKoLRh2LdG+3bMuNavnej5q74QjkUyvOqqplC30cJwqQpFlU1V+OOgapRVX1+FHHIGl8mNRIkiRJTYuzn406AEmSJElaHSY1kiRJkppm+ZkkSZLUssnZz8aYSY3WmC3XW6eWbrDO3A21IK649bGjDkGSpLFwI5dyW1033lnEImNSozVm6QbrsHyXB406jLH116efOeoQpJFYstL7Cklr15Es+AzFmieTGkmSJKllwdnPRh2AJEmSJK0OkxpJkiRJTbP8TJIkSWqaL98c77OXJEmS1DyTGkmSJElNs/xMkiRJalmAJeM9vb0jNZIkSZKaZlIjSZIkqWmWn0mSJEmtc/YzSZIkSWqXSY0kSZKkpll+JkmSJLUswDrOfiZJkiRJzTKpkSRJktQ0y88kSZKkpsXZz0YdgCRJkiStDpMaSZIkSU0zqZEkSZLUNJ+pkSRJklrmlM6O1EiSJElqm0mNJEmSpKZZfiZJkiS1bsl4j1WM99lLkiRJap5JjSRJkqSmWX4mSZIktSxx9rNRByBJkiRJq8OkRpIkSVLTLD+TJEmSWhZgnfEeqxjvs5ckSZLUPJMaSZIkSU2z/EySJElqnbOfSZIkSVK7TGokSZIkNc3yM0mSJKllCSwZ77GK8T57SZIkSc0zqZEkSZLUNMvPJEmSpNY5+5kkSZIktcukRpIkSVLTLD+TJEmSWhZgnfEeqxjvs5ckSZLUPJOaEUlyeJI3Dix/M8k/DywfluSAJOsmuTbJe6bsf1KSZVPW7ZHkqwPLf5fkG0k2GGyf5NIkXxho98IkRw0s75Xke0l+kGRFkmOTbLdGL4AkSZK0hpjUjM4pwO4ASZYAWwI7DGzfHTgVeAZwCfCiJENPa5Hk7cATgedX1R3TNHlcksdMs9+OwAeBl1fVb1bVLsBngaXDHluSJElr2ZIszs/aOv21diRNdSqwW/99B+AC4OYkmyfZAHg0cDawL/B+4OcD7WeV5EDgWcA+VXX7DM0OAw6eZv1bgUOr6qLJFVV1fFWdPMyxJUmSpLXNiQJGpKquSHJ3X9a1O3AasC1d4nITcD5d0vl04NXAZnQJzqlzdP1E4FHA46rqllnaHQe8NsnDp6zfAXjfPE9HkiRJGhlHakbrVLqEZjKpOW1g+RRgb+A7/WjLF4DnJVlnjj5/RDcHxjPmaLcSeC/wtpkaJNmif6bmkiQHzdDmVUmWJ1l+7d0TcxxSkiRJa1zSzX62GD9riUnNaE0+V7MTXfnZ6XQjNZPP0+wLPD3JpcBZwBbA0+bo82rg2cARSZ46R9vPAE8GHjyw7kJgV4Cqur5/puZIYOPpOqiqI6tqWVUt22pdf50kSZK09nkXOlqn0o3G3FBVK6vqBroys92AFcCTgO2qamlVLQVeR5fozKqqLgFeABydZJdZ2t0FHA68aWD13wMHJ3n0wLqN5nVWkiRJ0lpkUjNa59PNenb6lHU3AU8Fvj1l5rJ/A/bpJxIAOCHJZf3n84MdV9WZwCuA45M8bJYYPs7As1VVdT7wBuDTSS5OcgrdpAWfW6UzlCRJ0sIb9SxnI579zIkCRqiqVgL3m7Ju/4HFT03ZdgOwVb+4xwzdnjTQ/kRg8v0yewysXzrw/Q5gmynHOQE4Ya74JUmSpMXAkRpJkiRJTXOkRpIkSWpZWKszjS1G4332kiRJkppnUiNJkiSpaZafSZIkSU1buzONLUaO1EiSJElqmkmNJEmSpKZZfiZJkiS1zNnPHKmRJEmS1DaTGkmSJElNs/xMkiRJap2zn0mSJElSu0xqJEmSJDXNpEaSJElqWdLNfrYYP3OGnk8kuSbJBdNsOzBJJdlyrn5MaiRJkiSNylHAXlNXJnkw8HvAz4fpxKRGkiRJ0khU1cnADdNsOhx4C1DD9OPsZ5IkSVLrFu/sZ1smWT6wfGRVHTnbDkmeC1xeVecmw52XSY0kSZKkhXJdVS0btnGSjYC/pCs9G5rlZ5IkSZIWi4cB2wPnJrkU+A3g7CQPmm0nR2okSZKkloWhZhprQVWdDzxgcrlPbJZV1XWz7XfvOHtJkiRJzUlyDHAa8KgklyV55ar040iNJEmSpJGoqn3n2L50mH5MaiRJkqSmZTHPfrZWWH4mSZIkqWkmNZIkSZKaZvmZJEmS1LIAS8Z7rGK8z16SJElS80xqJEmSJDXN8jNJkiSpdes4+5kkSZIkNcukRpIkSVLTLD+TJEmSWpY4+9moA5AkSZKk1WFSI0mSJKlplp9JkiRJrVvi7GeSJEmS1CyTGkmSJElNs/xMkiRJalnw5ZujDkCSJEmSVodJjSRJkqSmWX4mSZIktW7MX75pUqM15sL77ciOe54y6jDG1pf++a9GHcLYOuz3/3bUIYy1B/1w1BFIkkZtvFM6SZIkSc1zpEaSJElqWcKEL9+UJEmSpHaZ1EiSJElqmuVnkiRJUsMKmBjz2c/G++wlSZIkNc+kRpIkSVLTLD+TJEmSGufsZ5IkSZLUMJMaSZIkSU2z/EySJElqWCWsXGe8xyrG++wlSZIkNc+kRpIkSVLTLD+TJEmSGufsZ5IkSZLUMJMaSZIkSU2z/EySJElqWaCWjPdYxXifvSRJkqTmmdRIkiRJaprlZ5IkSVLDCmc/c6RGkiRJUtNMaiRJkiQ1zfIzSZIkqWWJ5WejDkCSJEmSVodJjSRJkqSmWX4mSZIkNayb/Wy8xyrG++wlSZIkNc+kRpIkSVLTLD+TJEmSGufsZ5IkSZLUMJMaSZIkSU2z/EySJElqWCWszHiPVYz32UuSJElqnkmNJEmSpKaZ1EiSJElqms/USJIkSY1zSmdJkiRJaphJjSRJkqSmWX4mSZIkNc7yM0mSJElqmEmNJEmSpKZZfiZJkiQ1rAK1ZLzHKsb77CVJkiQ1z6RGkiRJUtMsP5MkSZKaFmc/G3UAw0hyeJI3Dix/M8k/DywfluSAJOsmuTbJe6bsf1KSZVPW7ZHkqwPLf5fkG0k2GGyf5NIkXxho98IkRw0s75Xke0l+kGRFkmOTbDfLufxaLEmWJrlgIKab+n4uSvKOfv1GST6b5PwkFyT5bpKH9O1WJLkqyeUDy+sn2TLJXUleM+X4lybZsv++sm9/QZKvJNmsX78kyQf69ecnOTPJ9nP8mCRJkqSRaCKpAU4BdofuhhvYEthhYPvuwKnAM4BLgBclGTpdTfJ24InA86vqjmmaPC7JY6bZb0fgg8DLq+o3q2oX4LPA0mGPPY3/6vtZBrwsya7AG4Crq2qnqtoReCVwVVXt0rf9KHD45HJV3Qm8CDgd2HeWY93et98RuAF4Xb/+JcA2wGOraifg+cCNq3FOkiRJ0oJpJak5Fdit/74DcAFwc5LNk2wAPBo4m+4G/v3AzwfazyrJgcCzgH2q6vYZmh0GHDzN+rcCh1bVRZMrqur4qjp5mGPPpqpuBc4CHg5sDVw+sO3iGZKvQfsCBwLbJvmNIQ55GrBt/31r4MqqmuiPd1lV/XKepyBJkqS1ITCxZMmi/KwtTSQ1VXUFcHdf1rU73Q34GXSJyzLgfLpzeTrwFeAYZh+hmPRE4DXAs6rqllnaHQfsmuThU9bvQJdMrXFJtgCeAFwIfAJ4a5LT+jK5R8yx74OBravqe3Sxv2SO9usAewLH96uOA/bpS9MOS/Jbs+z7qiTLkyxfedt1Q5+fJEmStKY0kdT0TqVLaCaTmtMGlk8B9ga+04+2fAF4Xn+zPpsfAaErW5vNSuC9wNtmapBkiz4JuCTJQbP0VXOse1KSc4ATgfdU1YVVtQJ4aB/D/YEzkzx6lmO8hC4xAfgXZk7w7pNkBXAV8EDgW9CNzACPojvfCeA/kuw57clUHVlVy6pq2TobbTlLSJIkSdLCaGn2s8nnanaiKz/7BV151a+ATwIvB343yaV9+y2Ap9HfqM/gamA/upv2G6rqO7O0/QzdTf4FA+suBHYFzq2q64Fd+oRm41n6uR7YfGD5/sDgEMd/VdXeU3fqR5K+CHwxyQTwbOCiqe16+wIPSrJfv7xNkkdU1Q+ntLu9qnZJshHwTbpnaj7QH+8O4OvA15NcDTwP+I9ZzkuSJEkjUMDE8I+T3yu1NlKzN3BDVa2sqhuAzehK0FYATwK2q6qlVbWU7gZ9zhK0qroEeAFwdJJdZml3F3A48KaB1X8PHDxl1GSjOQ55Et0EAJO/eS8HZkumSPLEJJv339cHHgP8bIa2jwQ2rqptB67Fu5nlWlTVbcDrgQP7GeR2TbJN398S4LEzHU+SJEkatZaSmvPpZj07fcq6m4CnAt+e8vD8v9E9F7JBv3xCksv6z+cHO66qM4FXAMcnedgsMXycgdGtqjqfbmayTye5OMkpdJMWfG6WPo4EbgbOTXIu3ajO+2ZpD/Aw4D+TnA+cAyynK7Gbzr7Al6as+wJzJHhVdQ5wXt/uAcBX+qmmzwPuBj40R4ySJEnSSDRTflZVK4H7TVm3/8Dip6ZsuwHYql/cY4ZuTxpofyIw+X6ZPQbWLx34fgfdVMeDxzkBOGGu+Afa3wn82QzbThqMaWD9p4FPz9LnIQPf/2aa7efRJVtTz2fjKe32GVj8xkzHkyRJ0uLiyzclSZIkqWHNjNS0JsmH6aaMHvT+qvrkKOKRJEmS7q1MahZIVb1u1DFIkiTp3q+Stfqiy8VovM9ekiRJ0sgk+USSa/oJqibXvTfJD5Kcl+RLSTabqx+TGkmSJEmjchSw15R13wJ2rKrHApfQvStyVpafSZIkSY1b2ejLN6vq5CRLp6w7cWDxdOCFc/XjSI0kSZKkxeqPga/P1ciRGkmSJEkLZcskyweWj6yqI4fZMcnBdC+B/+xcbU1qJEmSpIYVLObZz66rqmXz3SnJ/sDewJ5VVXO1N6mRJEmStGgk2Qt4C/CUqrptmH0WbUonSZIk6d4tyTHAacCjklyW5JXAh4BNgG8lWZHko3P140iNJEmS1LRQ7c5+tu80qz8+334cqZEkSZLUNJMaSZIkSU2z/EySJElqWWBiSZvlZ2uKIzWSJEmSmmZSI0mSJKlplp9JkiRJDStgIuM9VjHeZy9JkpZoIi0AACAASURBVCSpeSY1kiRJkppm+ZkkSZLUOGc/kyRJkqSGmdRIkiRJaprlZ5IkSVLLEiZi+ZkkSZIkNcukRpIkSVLTLD+TJEmSGlbAyiXjPVYx3mcvSZIkqXkmNZIkSZKaZvmZJEmS1DhnP5MkSZKkhpnUSJIkSWqa5WeSJElSwwrLzxypkSRJktQ0kxpJkiRJTbP8TJIkSWpZQvnyTUmSJElql0mNJEmSpKZZfiZJkiQ1ztnPJEmSJKlhJjWSJEmSmmb5mdaY+964hGVf2WDUYYyt1/3wHaMOYWy9+YwPjDqEsXbK/d8w6hAkaaR8+aYjNZIkSZIaZ1IjSZIkqWmWn0mSJEmNs/xMkiRJkhpmUiNJkiSpaZafSZIkSQ2rhImM91jFeJ+9JEmSpOaZ1EiSJElqmuVnkiRJUuOc/UySJEmSGmZSI0mSJKlplp9JkiRJDStg5RLLzyRJkiSpWSY1kiRJkppm+ZkkSZLUMl++6UiNJEmSpLaZ1EiSJElqmuVnkiRJUuPKl29KkiRJUrtMaiRJkiQ1zfIzSZIkqWEFTGD5mSRJkiQ1y6RGkiRJUtMsP5MkSZIaN+HsZ5IkSZLULpMaSZIkSU2z/EySJElqWpjIeI9VjPfZS5IkSWqeSY0kSZKkpll+JkmSJDWscPYzR2okSZIkNc2kRpIkSVLTLD+TJEmSWhZYafmZJEmSJLXLpEaSJElS0yw/kyRJkhrm7GeO1EiSJElqnEmNJEmSpKZZfiZJkiQ1LUyM+VjFeJ+9JEmSpOaZ1EiSJElqmuVnkiRJUuPK2c8kSZIkqV0mNZIkSZKaZvmZJEmS1DBfvrmAIzVJDk/yxoHlbyb554Hlw5IckGTdJNcmec+U/U9KsmzKuj2SfHVg+e+SfCPJBoPtk1ya5AsD7V6Y5KiB5b2SfC/JD5KsSHJsku1mOZejkvw0yblJLkny6SS/MbB9037dj5L8uP++ab/tS0meN9D24iRvH1j+QpIX9OdWSfYZ2PbVJHv03/dOck4fw/eTvDrJwX38K5KsHPj++n6fI5JcnmTJQJ/7J/lQ//2QfvuKvs99B9o9IckZ/baLkhwy0/WRJEmSRmkhy89OAXYH6G+qtwR2GNi+O3Aq8AzgEuBFyfApZp8YPBF4flXdMU2TxyV5zDT77Qh8EHh5Vf1mVe0CfBZYOsch31xVOwOPAs4Bvp1k/X7bx4GfVNXDq+phwE+ByQRu8DpsAdwK7DbQ72501wHgMuDgaWJeDzgS2KeP4beAk6rqXVW1S38Ot09+r6oP9Nf8+cAvgKfMcl6H9/s/F/in/lgAnwJe1W/bEThujusjSZIkjcRCJjWncs/N+w7ABcDNSTZPsgHwaOBsYF/g/cDP+fWb/RklORB4Ft1N/u0zNDuMaRIE4K3AoVV10eSKqjq+qk4e5tjVORy4CnhWkocDjwP+dqDZO4FlSR5Gdx1279fvDnwF2Cqd7emSkav67ecCNyV5xpTDbkJXKnh9H8MdVXXxHKHuAVwI/CPdNZ7rvH4I3AZs3q96AHBlv21lVX1/rj4kSZKk+UjyiSTXJLlgYN39k3wryQ/7fzefrQ9YwKSmqq4A7u7LunYHTgPOoEtclgHn98d/Ot2N/jEMcfNNNzrzGuBZVXXLLO2OA3btk45BO9AlU6vrbOA3gccAK6pq5eSG/vuK/lhnATv2ozqT1+FiuqRucrRq0LuAtw+uqKobgOOBnyU5Jsl+gyVlM9iX7pp+CXjOwAjMtJLsCvywqq7pVx0OXNyXz706yYYz7PeqJMuTLP/vu6+dIyRJkiQthAmyKD9DOArYa8q6vwD+o6oeAfxHvzyrhZ79bHKUYvJm/rSB5VOAvYHv9KMtXwCel2SdOfr8ERC6srXZrATeC7xtpgZJtuifGbkkyUFDnM+v7T5Mo7407kJgV+AJdInd1Osw2P7kPrbfnbL+T4A9ge8BBwGfmDGwLoF6NvDlqvpVf8xnztD8TUku7Nu8a+B476RLPk8EXgp8Y4bzO7KqllXVsg3X3WqmkCRJkqT/pb/3vWHK6ufSPQpB/+/zmMNCJzWTz5PsRFd+djrdSM3kCMW+wNOTXEo3orEF8LQ5+rya7ob9iCRPnaPtZ4AnAw8eWDeZYFBV1/fPjBwJbDz0WXV+C7gI+D6wy5SH8ZcAu/TboLsOTwY2qapf0l2HyaRm6kgNTDNa08d7fl/69gzgD2aJ7ZnAZsD5/bX9XWYeBTu8qnbo+/v44IhMVf24qv6RLpnauX8mSJIkSVpID6yqK/vvVwEPnGuHtTFSszdwQ/9cxg10N9u70ZVnPQnYrqqWVtVS4HUM9/zHJcALgKOT7DJLu7voyqjeNLD674GDkzx6YN1Gw55Q/yzM64GtgW9U1Y/oJg4YTELeDpzdb4PuOrya7pkZgPPoRm22o0v2psZ9It2zLY/tj7nx5CxovV2An80S5r7Anwxc1+2BZySZ8Tyr6nhgOfDy/pjPGZi44RF0I183znJMSZIkjUARJrJkUX6ALScfVeg/r5rXuVUV3azVs1ropOZ8ulnPTp+y7ibgqcC3p8xc9m/APv1EAgAnJLms/3x+sOOqOhN4BXB8/0D+TD7OwPt4qup84A3Ap/vplU+he77lc3Ocy3uTnEs3U9tvA0+tqjv7ba8EHtlP5/xj4JH9ukmnAg+lKzujqu4GrgGWV9XEDMd7F/eMMAV4Sx/vCuBvgP2n26lPXPYCThg451uB7wL7TLfPgHcCB/QjTX9E90zNCroRr/0GnxuSJEmShnDd5KMK/efIIfa5OsnWAP2/18zRfmFfvtnfBN9vyrr9BxY/NWXbDcDkgxl7zNDtSQPtT6Qb7fi19v3oxOT3O4BtphznBAZu+ucyJebptv8SeNks269hyjM4VbXHlOWT+PVzO37KPs+eI4aN+39vA+4/zfYXDCwe1a87ZEqbs+imrAb4w9mOJ0mSJC2Q4+mqh97T//tvc+2woEmNJEmSpIU35Exji06SY+gGJ7ZMchnwDrpk5rgkr6R75OLFc/VjUjMgyYfppowe9P6q+uQo4pEkSZLuzapqpufp95xPPyY1A6rqdaOOQZIkSdL8mNRIkiRJDavARNosP1tTFnr2M0mSJElaUCY1kiRJkppm+ZkkSZLUuJWNzn62pjhSI0mSJKlpJjWSJEmSmmb5mSRJktSwIs5+NuoAJEmSJGl1mNRIkiRJaprlZ5IkSVLjytnPJEmSJKldJjWSJEmSmmb5mSRJktS4iYz3WMV4n70kSZKk5pnUSJIkSWqa5WeSJElSwwqYGPPZz2ZMapJ8kO4aTauqXr8gEUmSJEnSPMw2UrN8rUUhSZIkSatoxqSmqj41uJxko6q6beFDkiRJkjS8jH352ZwTBSTZLcn3gR/0yzsn+ciCRyZJkiRJQxhm9rMjgGcC1wNU1bnAkxcyKEmSJEka1lCzn1XVL5JfG9JauTDhSJIkSZqvcS8/Gyap+UWS3YFKsh7wBuCihQ1LkiRJkoYzTPnZa4DXAdsCVwC79MuSJEmSNHJzjtRU1XXAfmshFkmSJEnzVMDKjHf52TCznz00yVeSXJvkmiT/luShayM4SZIkSZrLMOVnnwOOA7YGtgE+DxyzkEFJkiRJ0rCGmShgo6r6zMDy0UnevFABSZIkSZofZz+bQZL791+/nuQvgH+hK9l7CfC1tRCbJEmSJM1ptpGas+iSmMm079UD2wp420IFJUmSJEnDmjGpqart12YgkiRJkuavCBNDPSp/7zXMMzUk2RF4DLDh5Lqq+vRCBSVJkiRJw5ozqUnyDmAPuqTma8CzgO8CJjWSJEmSRm6YkZoXAjsD51TVK5I8EDh6YcOSJEmSNKwa89nPhim+u72qJoC7k9wPuAZ48MKGJUmSJEnDGWakZnmSzYCP0c2Idgtw2oJGJUmSJElDmjOpqarX9l8/muQbwP2q6ryFDUuSJEnSsHz55gyS7Drbtqo6e2FCkiRJkqThzTZSc9gs2wp42hqORY1b/3ZYes54z5E+StudN95/oRmlU455w6hDGGtv+9ar526kBbPTmYePOoSx9eJD7jPqEMZW3TXqCDTVbC/ffOraDESSJEnS/BWWn/lndUmSJElNM6mRJEmS1LRhpnSWJEmStIhZfjaHdF6W5K/75e2S/M7ChyZJkiRJcxum/OwjwG7Avv3yzcCHFywiSZIkSZqHYcrPHl9VuyY5B6Cqfplk/QWOS5IkSdIQirDS8rM53ZVkHbrZ4kiyFTCxoFFJkiRJ0pCGSWo+AHwJeECSdwHfBQ5d0KgkSZIkaUhzlp9V1WeTnAXsCQR4XlVdtOCRSZIkSRpKjXn52ZxJTZLtgNuArwyuq6qfL2RgkiRJkjSMYSYKOIHueZoAGwLbAxcDOyxgXJIkSZI0lGHKz3YaXE6yK/DaBYtIkiRJ0rz48s15qqqzgccvQCySJEmSNG/DPFNzwMDiEmBX4IoFi0iSJEmS5mGYZ2o2Gfh+N90zNl9YmHAkSZIkzUcBK2u8y89mTWr6l25uUlUHraV4JEmSJGleZnymJsm6VbUSeOJajEeSJEmS5mW2kZrv0T0/syLJ8cDngVsnN1bVFxc4NkmSJElDGPfZz4Z5pmZD4HrgadzzvpoCTGokSZIkjdxsSc0D+pnPLuCeZGZSLWhUkiRJkjSk2ZKadYCNYdqxLJMaSZIkaREoQll+NqMrq+qday0SSZIkSVoFM85+xvQjNJIkSZK0qMw2UrPnWotCkiRJ0iqbmHWs4t5vxrOvqhvWZiCSJEmStCrGO6WTJEmS1Lxh3lMjSZIkaRGbqPF+HN6RGkmSJElNM6mRJEmS1DTLzyRJkqSGFbByzN/G4kiNJEmSpKaZ1EiSJElqmuVnkiRJUtNCOfuZJEmSJLXLpEaSJElS0yw/kyRJkhpWwISzn0mSJElSu0xqJEmSJI1EkjcluTDJBUmOSbLhqvRj+ZkkSZLUsoKVDc5+lmRb4PXAY6rq9iTHAX8IHDXfvhypkSRJkjQq6wL3SbIusBFwxap0YlIjSZIkaa2rqsuB9wE/B64EbqqqE1elL5MaSZIkqWGTs58txg+wZZLlA59XTcadZHPgucD2wDbAfZO8bFWugc/USJIkSVoo11XVshm2PR34aVVdC5Dki8DuwNHzPYgjNYtUkoP7mSDOS7IiyeOTnJRkWZIz+nU/T3Jt//38JDf2369Kcnn/fUWS9ZPc0ve7NEkl+fOBY30oyf4Dywck+UHf57lJ/iHJeiO4DJIkSbr3+jnwhCQbJQmwJ3DRqnTkSM0ilGQ3YG9g16q6I8mWwPqT26vq8X27/YFlVfVnU/Y/BLilqt43sG6wyTXAG5L8U1XdOWXf1wC/Bzyhqm5Msj5wAHAf4K41dpKSJElaY6rB2c+q6owk/wqcDdwNnAMcuSp9mdQsTlvTDdXdAVBV18H/SkxWx7XAKcDLgY9N2XYw8OSqurE/9p3Ae9bUgSVJkqRJVfUO4B2r24/lZ4vTicCDk1yS5CNJnrIAx/h/wEFJ1plckeR+wMZV9dNhO0nyqskHv27j2gUIU5IkSZqdSc0iVFW3AI8DXkU3qnLs4DMva+gYPwHOAF46U5skz+yfybk0ye4z9HNkVS2rqmUbsdWaDFGSJElDGf0sZ7PMfrZWmNQsUlW1sqpO6ofk/gz4gwU4zKHAW6H7jauqXwG3JNm+X/5mVe0CXMDAMz2SJEnSYmJSswgleVSSRwys2gX42Zo+TlX9APg+sM/A6ncD/5hksz6WABuu6WNLkiRJa4oTBSxOGwMf7BOLu4Ef0ZWi/esCHOtddDNNTPpH4L7AGUnuAG6hm1TgnGn2lSRJ0ogVMNHg7GdrkknNIlRVZ9G9eGiqPaa0Owo4apr9D5lm3cb9v5cCOw6sP5eBEbuqKuC9/UeSJEla9Cw/kyRJktQ0R2okSZKkxq0c8/IzR2okSZIkNc2kRpIkSVLTLD+TJEmSGldr8UWXi5EjNZIkSZKaZlIjSZIkqWkmNZIkSZKa5jM1kiRJUsMKmHBKZ0mSJElql0mNJEmSpKZZfiZJkiS1rMJKy88kSZIkqV0mNZIkSZKaZvmZJEmS1LBu9rNRRzFajtRIkiRJappJjSRJkqSmWX4mSZIkNa6c/UySJEmS2mVSI0mSJKlplp9JkiRJDetmP7P8TJIkSZKaZVIjSZIkqWmWn0mSJEmNm8DyM0mSJElqlkmNJEmSpKZZfiZJkiQ1rICVzn4mSZIkSe0yqZEkSZLUNMvPJEmSpJZVKMvPJEmSJKldJjWSJEmSmmb5mSRJktS4iQnLzyRJkiSpWSY1kiRJkppm+ZkkSZLUMF++6UiNJEmSpMaZ1EiSJElqmuVnkiRJUssKJiw/kyRJkqR2mdRIkiRJaprlZ9K9xJKV4z3srPH1zM99aNQhjLVLePyoQxhbh9x59qhDGFuL8f9xy/IzSZIkSWqXSY0kSZKkpll+JkmSJDWsiLOfjToASZIkSVodJjWSJEmSmmb5mSRJktS4iRp1BKPlSI0kSZKkppnUSJIkSWqa5WeSJElSw6pg5YSzn0mSJElSs0xqJEmSJDXN8jNJkiSpceXLNyVJkiSpXSY1kiRJkppm+ZkkSZLUuAnLzyRJkiSpXSY1kiRJkppm+ZkkSZLUsMKXbzpSI0mSJKlpJjWSJEmSmmb5mSRJktSyirOfjToASZIkSVodJjWSJEmSmmb5mSRJktSwAmpi1FGMliM1kiRJkppmUiNJkiSpaZafSZIkSY1z9jNJkiRJaphJjSRJkqSmWX4mSZIktaxgYqLd8rMkmwH/DOxIN5nbH1fVafPpw6RGkiRJ0ii9H/hGVb0wyfrARvPtwKRGkiRJ0kgk2RR4MrA/QFXdCdw5335MaiRJkqSGFbCy3dnPtgeuBT6ZZGfgLOANVXXrfDpxogBJ+v/t3XmYZWV5rvH7qRYEBUSERCKYdoiiKBGEHAVFhjhEwdmoJBEMinoUFEUTFQVyRD1BlKgRg/MQ0SBxgJMoRkBAERnEMEVBBWQwEVBsaSLa/Z4/1qqwLasLuqr2Xr1q3b/r2hdr2MNTm+7q/e7vW+8nSZLGZfMk543cDphx/i7ADsCxVbU9cAvw12v7Io7USJIkSRqXG6pqxznOXwNcU1XntPufxaJGkiRJGp7qafezqvpxkh8leXBVfRfYE7h0bZ/HokaSJElSlw4E/rHtfPYD4IVr+wQWNZIkSZI6U1UXAnNNUbtDFjWSJElSjxWwurpO0S27n0mSJEnqNYsaSZIkSb3m9DNJkiSpzyqs6mn3s8XiSI0kSZKkXrOokSRJktRrFjUTkuSNSS5J8u9JLkxyWvvfK5Lc3G5fmGTn9v4XJvn0jOf4aJJrk9y13d88yZXt9vIktyb5dpLLknwryX4jj90vyXvb7cOTrEzyOyPnfzGy/btJPpXkB0nOT3J2kmeM8/2RJEnS/BSwenXWydukeE3NBCR5NLAXsENV/TLJ5sD6VXVdkt2AQ6pqr5H7PwRYBjw2yd2r6paRp1sF/CVw7Cwv9f2q2r59jvsD/5wkVfWRWe57A/Aa4K9mZA3weeBjVbVPe+z3gafO52eXJEmSxs2RmsnYErihqn4JUFU3VNV1c9z/+cAngFOAp804dwxwcJI5C9Kq+gHwauCgNdzlw8Bzk2w24/gewG1V9f6R57qqqt4z1+tJkiRJXbGomYxTgK2TfC/J+5I87g7u/1zg08DxNAXOqKuBs4C/uBOvewGwzRrO/YKmsHnljOPbto+TJElST1RlnbxNikXNBFTVL4BHAgcAPwE+M3q9y6gkO9KM6lwNfBXYfpbRlLcBr+WO///d0Z+kdwP7Jtl4jU+Q/H2S7yQ5dw3nD0hyXpLzVvKTO3g5SZIkafFZ1ExIVa2qqtOr6jDgFcCz1nDX5wPbtA0Avg9sMvO+VXU5cCHwp3fwstsDl82R6WfAp4CXjxy+BNhh5D4vB/YEtljDcxxXVTtW1Y53m/0ukiRJ0lhZ1ExAkgcn+YORQ48ArprlflM0hcrDq2p5VS2nuaZm5hQ0gCOBQ+Z4zeXAO4A7uhbmncBLuL1pxKnABkleNnKfu93Bc0iSJKkrBatXr5u3SbH72WRsBLwnyabAr4EraKaizfRY4NoZTQTOAB6aZMvRO1bVJUkuYGRUBXhAkm8DGwArgHdX1UfnClZVNyT5HHBwu19Jng68K8nraKbL3cKMLmmSJEnSusKiZgKq6nxg5zWcOx04vd3+GvCoGedXAfdud/ebce6ZI9tXAhvOkeGjwEfb7cNnnHs1Tae06f3rgeet6bkkSZKkdYlFjSRJktRj04tvDpnX1EiSJEnqNYsaSZIkSb3m9DNJkiSpzwpWOf1MkiRJkvrLokaSJElSrzn9TJIkSeqxInY/6zqAJEmSJC2ERY0kSZKkXnP6mSRJktRztbrrBN1ypEaSJElSr1nUSJIkSeo1p59JkiRJfVawqux+JkmSJEm9ZVEjSZIkqdecfiZJkiT1WIGLb3YdQJIkSZIWwqJGkiRJUq85/UySJEnqudUuvilJkiRJ/WVRI0mSJKnXLGokSZIk9ZrX1EiSJEl9VlC2dJYkSZKk/rKokSRJktRrTj+TJEmSeqyA1U4/kyRJkqT+sqiRJEmS1GtOP5MkSZL6rGDV6q5DdMuRGkmSJEm9ZlEjSZIkqdecfiZJkiT1WBG7n3UdQJIkSZIWwqJGkiRJUq85/UySJEnqs4Ja5fQzSZIkSeotixpJkiRJveb0M0mSJKnHChffdKRGkiRJUq9Z1EiSJEnqNaefSZIkST3n4puSJEmS1GOO1EiSem23j6zXdYRBO3T987uOMFhvYdjfzHfp5K4D6LdY1EiSJEl9VrDa7meSJEmS1F8WNZIkSZJ6zelnkiRJUs9lHe1+VhN6HUdqJEmSJPWaRY0kSZKkXnP6mSRJktRnBctWrZvTz349oddxpEaSJElSr1nUSJIkSeo1p59JkiRJPRZgysU3JUmSJKm/LGokSZIk9ZrTzyRJkqQ+qzC1ji6+OSmO1EiSJEnqNYsaSZIkSb3m9DNJkiSp57Kq6wTdcqRGkiRJUq9Z1EiSJEnqNaefSZIkST2WgmV2P5MkSZKk/rKokSRJktSZJMuSfDvJyfN9DqefSZIkST03tbrrBAvySuAyYJP5PoEjNZIkSZI6kWQr4CnABxfyPBY1kiRJksZl8yTnjdwOmHH+GOB1wILGmpx+JkmSJPVYCqZWrbPdz26oqh1nO5FkL+C/qur8JLst5EUcqZEkSZLUhV2Apya5Evg0sEeST87niSxqJEmSJE1cVb2+qraqquXA84BTq+rP5/NcTj+TJEmSei4DX3zTokaSJElSp6rqdOD0+T7e6WeSJEmSes2RGkmSJKnHUrBsVdcpuuVIjSRJkqRes6iRJEmS1GtOP5MkSZJ6LUwNvPuZIzWSJEmSes2iRpIkSVKvOf1MkiRJ6rOCKbufSZIkSVJ/WdRIkiRJ6jWnn0mSJEk9FiB2P5MkSZKk/rKokSRJktRrTj+TJEmS+qxgmd3PJEmSJKm/LGp6IMkv5jh3TJJrk0wl2SDJfyR5+Mj51yb5hyTLk1zcHtstSSXZe+R+JyfZrd2+S5K3Jrk8yYXt7Y1j/BElSZKkebOo6bEkU8AzgB8Bj6uq/wZeBbwvjfsALwX+epaHXwOsqVB5C/B7wMOr6hHAY4H1Fju/JEmSFi7A1Op18zYpFjX9thtwCXAs8HyAqvoScD3wAuBdwOFV9dNZHvsd4OYkjx89mORuwIuBA9siiapaUVWHj+lnkCRJkhbEoqbfng8cD3wOeEqS6dGUVwFHAltU1SfmePyRwKEzjj0QuLqqVtyZAEkOSHJekvNW8pO1Sy9JkiQtAouankqyPvBk4PNV9XPgHOCJAFV1HXAqzQjOGlXVGe1zPWaO13lhe03Nj5JsPctzHFdVO1bVjndji/n/QJIkSZqfgqlVWSdvk2JR019PBDYFLkpyJfAY2ilordXt7Y7MHK25Arhvko0Bquoj7XU1NwPLFiG3JEmStKgsavrr+cCLqmp5VS0H7gc8vr0m5k6rqlOAewLbtfsrgQ8B702yAUCSZcD6i5hdkiRJWjQuvtkPd0tyzcj++4An0XQ2A6CqbklyFrA38Jm1fP4jgS+M7L8R+D/AxUlWALcCHwOum0d2SZIkjVkm2GlsXWRR0wNVNduI2ltnud8zR7b3m3HuSuBh7fbpwOkj575I0w1wev9XNG2gZ2sFLUmSJK1TnH4mSZIkqdccqZEkSZJ6LAXLJthpbF3kSI0kSZKkXrOokSRJktRrTj+TJEmSem5qVdcJuuVIjSRJkqRes6iRJEmS1GtOP5MkSZJ6LAVTq+1+JkmSJEm9ZVEjSZIkqdecfiZJkiT1XOx+JkmSJEn9ZVEjSZIkqdecfiZJkiT1WYVlq+x+JkmSJEm9ZVEjSZIkqdecfiZJkiT1WAqm7H4mSZIkSf1lUSNJkiSp15x+JkmSJPXc1OquE3TLkRpJkiRJvWZRI0mSJKnXnH4mSZIk9VlBXHxTkiRJkvrLokaSJElSrzn9TJIkSeqxAMtcfFOSJEmS+suiRpIkSVKvOf1MkiRJ6rOCKaefSZIkSVJ/WdRIkiRJ6jWnn0mSJEk9FmDKxTclSZIkqb8saiRJkiT1mkWNJEmSpF7zmhpJkiSpzwqyuusQ3XKkRpIkSVKvWdRIkiRJ6jWnn0mSJEk9FmDZqq5TdMuRGkmSJEm9ZlEjSZIkqdecfiZJkiT1WcHUqnSdolMWNVo013P+DUeQq7rOsQCbAzd0HWKgfO+75fvfnf6/97d1HWBBev3+H9l1gIXp9XsP/H7XAfSbLGq0aKpqi64zLESS86pqx65zDJHvfbd8/7vje98t3//u+N5rsVnUSJIkSX1WMGX3M0mSJEnqL4sa6XbHdR1gwHzvu+X73x3f+275/nfH916LKlXVdQZJkiRJ87TJ5o+snfb6JUXk3wAAHcRJREFUZtcxZnXqx9Y/fxLXTzlSI0mSJKnXLGokSZIk9ZrdzyRJkqQ+c/FNixpJGrok9wR+Vl5kKWkCktwHWNbuXldVv+4yj5YGp59pcJJsleQxI/uvTvLm9vbALrMNQZLfT3KPkf3dk/xd+/9h/S6zDUH753ybdvuuSU4Dvg/8Z5I/7jbd0pZkWZKNRvYflWTX9rZxl9mGIMnTkrx8ZP+cJD9ob8/uMttSl+T1Sd48cuhs4GTgFOC13aTSUmNRoyE6Cth0ZP8lwC1AAUd0kmhY/gm4O0CSRwAnAFcDfwi8r8NcQ/Fc4Lvt9r7tf7cAHge8tZNEw/F/gf89sn88zQe6NwGHdpJoWF4HfHFk/67ATsBuwMu6CDQgzwGOHtm/saq2A7YFntJNpKUlNItvrou3O8yebJ3ktCSXJrkkySvn8x44/UxD9OCqOnlkf2VVHQ2Q5MyOMg3JhlV1Xbv958CHq+roJFPAhR3mGorbRqaZPRH4dFWtAi5L4r8J47UnzYfoaT+rqr2TBPB3z/itX1U/Gtk/q6puBG5McveuQg1FVd0ysvt37bFVSTbsKJLWHb8GXlNVF7Sj1ucn+UpVXbo2T+JIjYZogxn7e45sbz7JIAM1eiXjHsBXAapqdTdxBueXSR6WZAtgd5rpH9Pu1lGmoZiace3AXwG0ReZGsz9Ei+ieoztV9YqR3S0mnGVoNkqy3vROVX0UmimwwCZdhdK6oaqur6oL2u0VwGXAfdb2efxWTkO0IsmDqup7AFV1E0B7ncGKTpMNw6lJ/gm4nuZDxqkASbYEbusy2EC8EvgszYe4d1XVDwGSPBn4dpfBBmD9JBu3/2hTVacAtNeYzfyyRYvvnCQvrqoPjB5M8hLgWx1lGorPAv+Q5BVVtRKgHR17b3tOC1V3bqrXui7JcmB74Jy1faxFjYboMODkJEcCF7THHgm8geYDn8brVTTXdWwJPKaqftUevzfwxs5SDURVnQNsM8vxfwH+ZfKJBuUDwGeSvLSqroamcQZwLPDBTpMNw8HA55Psw2/+7r8r8PTOUg3Dm4AjgauTXEUzYr818KH2nJa2zZOcN7J/XFUdN/NObSOVE4FXVdXP1/ZFYgdPDVGSh9FcNLpte+hi4Kiquri7VMPWXlPz/Kr6x66zLHVJlgH3rKob2v31gf2Ag6vqIV1mW+qSvJTmC5S703ywWwG8vaqO7TTYgCTZg9t/919SVad2mWdI2utnpruMXlFVt3aZZym5x2aPrJ0fv9aDGxPxpX9a7/yq2nGu+7TTE08GvlxV75zP6zhSo0Fqi5cXdJ1jiJJsArycZr7sF4GvAK8AXgN8B7CoGaMkzwP+AbglyeU0355+GDgX+LMusw1BVb0feP90C+fpqWianLaIsZCZoCS7znJ4p6ZHBlTVGZNNtAT1ePpZ2yzlQ8Bl8y1owKJGA5TkIzTtm2dTVbX/JPMM0CeAn9KsU/Aimm+tAzy9qux+Nn6HAo+sqiuS7EDz/+HZVXVSx7mWvCS/9UXK9Ic6gKr6+EQDDUySFcz+u/8uNJ3R/Ew0PrOtRVPAdjTT0JbNcl7DsQvwF8BFSaY/B7yhnRZ9p/kXWEN08izHtqaZb+0v1vG7f1U9HCDJB2kaBty3qv6721iDcVtVXQHQts+83IJmYnZaw/Gn0oxcWtSMUVX9xgKn7fz9l9OsVfa5TkINRFXtPbqfZBeaL1h+DBzYSSitM6rqLH6zM+q8WNRocKrqxOntJPenGSnYFXg7zfCnxmu6McD0GgXXWNBM1O8kefXI/qaj+wsZ+tfcqup/Pry10y3+jKat8zdppgFqApJsStOw5AXAp4Cd2vVqNGZJ9qRpDFDAW6vqKx1HWjJCmFq14Lqg1yxqNEht++ZDadoGHgW8dMb6ERqfP0wy3dUkwIbtfmim/7lmwXh9ANh4jn2NUbvA6X7AITTFzLOr6rudhhqIJJvTXLv3XJrryLavqpu7TTUMSZ5C093yZuDQ9pt5aVFZ1GhwkpxA08bzaJopZ6uATUYuWLypu3RLX1U5xa9DVXVE1xmGKsnLadrGfxV4UlVd2W2iwbkK+AnwEWAlsP+Ma5ocpRyfk4BrgBuB1yV53ejJqnpqJ6m0pFjUaIh2ohn6PoTmW7vR8doC7t9FqKFIstlc5y0qxyvJu+c6X1UHTSrLAL0H+C/gMcAuIx+op0cpt+sq2EAcxe2NAhydnKzduw6w5PW4+9lisajR4FTV8q4zDNz5NB8sZpv8a1E5fud3HWDA7td1gCGrqsO7zjBUVfW1rjNo6bOo0eC0bWzXqKoumOu8Fqaq/GDXoar62GzHk2wA7D3bOS2Oqrqq6wxD5ihld5JcxJqXUsBRSi0GixoN0dFznCtgj0kFUSPJA4B9gOdV1bZ3dH8tjiTLgCcCzweeAJwJnNBpqCVsjnVSbJIxGY5SdmevrgMsdXH6mUWNBumJVXXbbCeSOIowIUl+j6YL0T7Aw4G3Ac/rNNRAJHkczfv+ZOBbNAuf3a+qVnYabOnbrKp+dcd30zisaZRSE/HXwF9V1c/v8J7SPE11HUDqwOeTrD/zYJLtgNM6yDMoSQ5IchpwOnAvYH/g+qo6oqou6jTcACS5hqaAPAt4aFU9C7jVgmYizuk6wNAl2TfJBUluaW/nJXlB17kG4AfA+Un26TqIli5HajREFwD/mmTv6Q9ySXYDPgm8sMtgA/Fe4Gxgn6o6DyDJGudaa9F9Fng6zSjZqiRfYI657lpUw14Zr2NJ9qVZdPPVNP8OBNgBOCpJVdUnusy3lFXVUUk+Bbwzyf7AscDqkfP/3Fm4JcTpZ9LAVNWhSQ4FvpzkT2iuJTgGePr0h2yN1ZbAc4Cjk9wb+CdgvW4jDUdVvSrJwcBuNNfS/C1wjyR/CvxLVf2iy3xL3BZJXr2mk66TMnYvA54xY32gU5M8C/g0YFEzRlV1bZL/BxxJ05RkuqgpwKJGC2ZRo0GqqrckWUlz4WiAParqio5jDcXNVfV+4P1JtqIZMfjPJJcBn6uqN3Qbb2lL8oqqei/NVMvTkqzH7c0C3gds3mW+JW4ZsBGO2HRlk9kWPK2qK5PYpGGMkmxLMzpzHfBHVXV9x5G0BFnUaHCSnMTt66RsAVxBMyQOuLLxBHyLZsoHVXUNTTe6o5M8CBsFTMJf0kwBBKC9cP1k4OQkG3aWahiur6q/6TrEgN06z3NauM8Cr6qqL48enG4lX1V2XVygpvvZsL8vsajREL1jDduajFl/61bV9wA/8HWoqvxgN17Luw4wcA9J8u+zHA8u+jtuj6iqX4Kt5DU+FjUanLlWNk7yGcCVj8fL6wq6tV2S2dqqulbK+F3TdYCBe0jXAYaqqn5pK3mNm0WN9Jse3XWAAfC6gm5dVFXbdx1ioH7ddYAhq6qrus4wVG0r+atprqs5pKpWJPmhBc3isvuZJE2W1xVoqLZK8u41nayqgyYZZmiS/JDfbF+ekf2qqgdMPtVg2EpeY2dRo8FJssOaTmFr4UlwhKZbzl3vzq00HRfVjR1n7E8BfwocAnx78nGGw1bymgSLGg3R0XOc+4+JpRiupyVZr+26RZIH08yxvsoF2CbiJ0n+oKouT9Py78PAs4Argf2q6oJO0y1tN1bVx7oOMVRVdSNAkingL4DXAhcCT6mqS7vMNgRVVdhKfnzK6WcWNRqcqtp9TefaX7Qar08C+wOXJ3kgcDbwj8BeSXaqqtd3mm7peyXw0Xb7+cB2wP2A7YG/Ax7bTaxBuK3rAEPW/n7/S+Bg4CyaBZddn6wDM1rJ+ztfi8KiRoPXflu9B01Xlr2A3+020ZJ3z6q6vN3eFzi+qg5Msj7N1Bz/gRuvX0+PktH8ef94+w32vyX52w5zDcHL55j+iqNkY/dDmmYNx9BctL5dku2mTzpS3JmXAW/rOoT6z6JGg5XkUTSFzNOBzYCX08yt1niNXhy6B3AUQFXdlmR1N5EGZXWSLYGfAnsCR46cc/HN8XoHty/8C799ofQek40zOP9G857/YXsbVYBFTTe8znIRxOlnFjUaniRvBZ5D803d8cARwHnOdZ+Yf0/yDuA64IHAKQBJNu001XC8GTiPprX2F6vqEoB2DYkfdBlsAP4K+FFVXQ+QZF9uv57p8O5iDUNV7bemc0kcoe+OXdC0KKa6DiB14EXAf9L0y/9EO/XGX6qT82LgBuC+wBNG1il4KM032RqjqjoZ+H3gIVX14pFT59G0W9X4vB+YXlV9V5opNx8DbgaO6zDXICXZNMn+Sb6K3c/GKsmKJD+f5bYC+L2u82lpcKRGQ7Ql8Hiai6SPSXIasGGSu1SVi+ONWVXdmuRLNKM0t40c/wbwjc6CDUT7YXp6e7a7nDG5NIOzrKpuarefCxxXVScCJya5sMNcg5FkQ+BpNFOPtwc2ppmC7J/7MaqqjbvOMAROP5OG50CaD8/700zB2YvmWoJrk3y1qvbpMtxSl+TNwJ8BFwB/m+RtVfWBjmMNyWtnOVY0XdC2pvk7ofFYNvLlyZ7AASPn/Pd4zJJ8iqa73ynAe4BTgSuq6vQuc0laHP4S1RBtRdP9ZhvgIuDrNC1upxcG03g9F9i+qlYmuRfwJcCiZkKqau/R/SS7AIcCP6Yp+DU+xwNfS3IDzUKcZwK0rc1v7jLYQDyUpkHGZcBlVbUqiVOPpSXCokaDU1WHALQthHcEdgZeCDya5oPFJ7pLNwi/nL6OpqpubBfC04Ql2RN4E80ozVur6isdR1ryqurI9vqNLYFT2sUIobm+1YJyzKrqEUm2oZl6/G9tcblxkt+tqv/sOJ60IE33s2E3krOo0ZBtCGwC3KO9XUczcqPxun+SL7bbAR4wsk9VPbWbWMOQ5CnAG2kK+EOr6qyOIw1KVX1zlmPf6yLL0CR5VPv+HwYcluSRNAXOuUmuqaqdu00oaSFy+xdF0jAkOQ7YFlgBnAN8E/hmVf2002AD0bYOXqOq+tqksgxRuxbQNcB3mKXrn0WllqokF1TVby1+2i7A/NiqslmAemvzu+9YT9n23K5jzOrj506dX1U7jvt1HKnREN0XuCtwOXAtzQe8n3WaaEAsWjq3e9cBpHVJOw3Qgka9Z/czaWCq6kntN3Pb0lxP8xrgYUluAs6uqsM6DbjEJbmI3xwhKJp1a04D3lFV/91JsOG4J/CNqvqvroNIEzY69fW3OEop9ZtFjQap/Wbu4iQ/o7m24Gaa1s5/RDPfWuOz1yzHNgP2pWmz+uJZzmvx/Dnw90lW0rQ2/zpNkXNxt7GksfsJcHTXISSNh0WNBifJQTQjNDsDv6L5YPcN4MPYKGDsquqqWQ5fBXw7iat6j1lVPRsgyXJu/3vwkiT3Bc6tqid3l04aq184/VVLVjn9zKJGQ7QcOAE4uKqu7ziLfpPtnSekqq5MsgFNF8ANgeltaan6aZJ7V9WPAZK8AHgWzZcqh1fVTZ2mk7QgFjUanKp6ddcZhizJb3UfornO48/xYt2xS/IGmjWZtgC+S9P9773AAVU18O/5tMRtCtwGkGRX4O006wM9AjgOeHZ30SQtlEWNpEmbOae9gBuB02k+WGi8XgDcApxEM+3ynKpyNXsNwdTIaMxzgeOq6kTgxCQXdphLWrA4/cyiRtJkVdWdaimcZN+q+ti48wxNVW2TZDOaa2l2A/46yUY069Z8o6o+0mU+aYzukuQuVfVrYE/ggNFzHWWStEicvy5pXfXKrgMsVVV1U1WdDLwZeD3NNWa7Ax/sNJg0XscDX0vyBeBW4EyAJA+k6YApqcf8ZkLSuipdB1iKkjyVZpRmF5q1mi6haev8GprpaNKSVFVHJvkqsCVwStvaH5oveA/sLpm0OJx+Jknrprrju2ge9qMpYl4HnF9Vt3UbR5qcqvrmLMe+10UWSYvLokbSusqRmjGoqmcCJLkf8IQkAJdW1Q86DSZJ0gJY1EhaV3296wBLUZKNgQ8Bj6RpDgDwiCTnA/tX1c87CydJmhe7n1nUSJqwJFsBy6vqrHb/1cBG7elPVdUVAFX1io4iLnXvAS4FnldVqwHSDNe8iWa9mhd0mE2SpHmx+5mkSTuKZhG8aS+hWTelgCM6STQsu1TV4dMFDUA1/oZmUU5JknrHkRpJk/bgtp3wtJVVdTRAkjM7yqSG1zFJUh8VTP266xDdcqRG0qRtMGN/z5HtzScZZKC+keTN7ZSz/5HkTcDZHWWSJGlBHKmRNGkrkjxouo1qVd0EkGQbYEWnyYbhQJpGAVckubA9tj1wAbB/Z6kkSVoAixpJk3YYcHKSI2k+SEPTiesNwCs7SzUQbXez5yR5APDQ9vDrqur7HcaSJC3Q1KphzyC2qJE0UVX1pSTPpFn88aD28MXAM6vq4u6SDUeSu9AUNNu0hyrJVVU18BnZkqS+sqiRNHFt8WLr4A4kuQ9wKnA98G2a5gB7AUcn2b2qrusynyRJ82FRI2miknyEpn3zbKqqvK5jvI4Ejq2qY0YPJjkIeBuwbyepJEnz5uKbFjWSJu/kWY5tDRwMLJtwliF6VFXtN/NgVb07yXc7yCNJ0oJZ1EiaqKo6cXo7yf1pGgTsCrydpiuXxuvWOc6tnFgKSZIWkUWNpIlr2zcfStNK+CjgpV6kPjH3aBs1zBRgk0mHkSQtDqefSdIEJTmBpoXz0TRTzlYBm0yvBTm9bo3G5mvA3ms4d8Ykg0iStFgsaiRN2k40jQIOAV5DM0IwrYD7dxFqKKrqhXfmfkn2raqPjTuPJEmLwaJG0kRV1fKuM+hOeSVgUSNJPWD3M4saSROWZIe5zlfVBZPKojkNe2lqSVKvWNRImrSj5zhXwB6TCqI5rWktIUmS1jkWNZIm7YlVddtsJ5Lcb9JhtEaO1EhSXzj9jKmuA0ganM8nWX/mwSTbAad1kEez+3rXASRJurMcqZE0aRcA/5pk76paCZBkN+CTwJ3qzKX5S7IVsLyqzmr3Xw1s1J7+VFVdAVBVr+gooiRJa82RGkkTVVWH0ozIfDnJRu1CkB8Hnl5VX+k23SAcBWw6sv8S4Baaa2iO6CSRJGnBplatm7dJcaRG0sRV1VuSrATOp7l2Y4/pEQKN3YOr6uSR/ZVVdTRAkjM7yiRJ0oJY1EiaqCQn0YwKBNgCuAJ4Z9Jcl15VT+0u3SBsMGN/z5HtzScZRJKkxWJRI2nS3rGGbU3GiiQPqqrvAVTVTQBJtgFWdJpMkjQvLr5pUSNpwqrqa2s6l+QzwBrPa1EcBpyc5Eiapg0AjwTeALyys1SSJC2ARY2kdcmjuw6w1FXVl9rmDK8DDmoPXww8s6ou7i6ZJEnzZ1EjSQPTFi8v6DqHJGmRFEz9uusQ3bKokTRRSXZY0ylgvUlmGaIkH6Fp1DCbqqr9J5lHkqTFYFEjadKOnuPcf0wsxXCdPMuxrYGDgWUTziJJ0qKwqJE0UVW1+5rOJXGkZsyq6sTp7ST3p2kQsCvwduBDXeWSJM1fsPvZVNcBJA1bGnsm+RBwTdd5hiDJNkk+CZwEnAU8tKqOrarbOo4mSdK8WNRI6kSSRyV5N3AV8AXgDGCbblMtfUlOAP4FOBvYDfgisEmSzZJs1mU2SZLmy+lnkiYqyVuB5wBXA8cDRwDnVdXHOg02HDvRNAo4BHgNzayFaQXcv4tQkqQFcPFNixpJE/ci4HvAscBJVfXLJGvqxqVFVlXLu84gSdJis6iRNGlbAo8Hng8ck+Q0YMMkd6mqgXfZH785WmoDUFUXTCqLJElJngT8HU0Hzg9W1dvn8zwWNZIm7UDgG8D+NL/A9gI2BK5N8tWq2qfLcAMwV0vtAvaYVBBJ0uLp4/SzJMuAv6f5svMa4NwkX6yqS9f2uSxqJE3aVsAxNE0BLgK+DnyUZp2U3TpLNRxPXFOXsyT3m3QYSdKg/RFwRVX9ACDJp4GnAWtd1Nj9TNJEVdUhVbUzcG/g9cBNwAuBc2jWTNF4fT7J+jMPJtkOOK2DPJKk4boP8KOR/WvaY2vNkRpJXdkQ2AS4R3u7jmbkRuN1AfCvSfauqpUASXYDPklTXEqSeuZ6zv/y4WTzrnOswQZJzhvZP66qjlvsF7GokTRRSY4DtgVW0IzOfAN4Z1X9tNNgA1FVhyY5FPhykj8BnkAzHfDpVXXe3I+WJK2LqupJXWeYp2uBrUf2t2qPrTWLGkmTdl/grsDlNL+4rgF+1mmigamqtyRZCZxPs07NHlV1RcexJEnDcy7wB+01ndcCzwPm1TAoVS4PIWmykoRmtGbn9vYwmmtrzq6qw7rMttQlOYmmy1mAXYArgB9Pn6+qp3YUTZI0QEmeTDNjYBnw4ao6cl7PY1EjqStJtqL5YL0zTWvne1XVpt2mWtqSPG6u81X1tUllkSRpsVjUSJqoJAdx+wjNr2iuqZm+XVRVqzuMN2hJPlNVz+06hyRJa8traiRN2nLgBODgqrq+4yz6TY/uOoAkSfPhSI0kCYAkV1fVfbvOIUnS2nKkRpIGJMkOazoFrDfJLJIkLRZHaiRpQJKcNtf5qtp9UlkkSVosFjWSJACSrFdVv+o6hyRJa2uq6wCSpO6ksWeSD9EshCpJUu9Y1EjSACV5VJJ3A1cBXwDOALbpNpUkSfPj9DNJGpAkbwWeA1wNHA98Djivqu7XaTBJkhbA7meSNCwvAr4HHAucVFW/TOK3W5KkXnP6mSQNy5bAW4C9ge8n+QSwYRK/5JIk9ZZFjSQNy4HATcD+wAOAzwNfB65N8qkug0mSNF8WNZI0LFsBxwD/BZwCPBL4KLAj8K/dxZIkaf5sFCBJA5RkfZpCZmfg0e3t5qp6SKfBJEmaB+dQS9IwbQhsAtyjvV0HXNRpIkmS5smRGkkakCTHAdsCK4BzgG8C36yqn3YaTJKkBfCaGkkalvsCdwV+DFwLXAP8rNNEkiQtkCM1kjQwSUIzWrNze3sYTUe0s6vqsC6zSZI0HxY1kjRQSbYCdqEpbPYC7lVVm3abSpKktWdRI0kDkuQgbh+h+RXwjZHbRVW1usN4kiTNi93PJGlYlgMnAAdX1fUdZ5EkaVE4UiNJkiSp1+x+JkmSJKnXLGokSZIk9ZpFjSTpTkuyKsmFSS5OckKSuy3guT6a5Nnt9geTPHSO++6WZOd5vMaVSTa/s8dn3OcXa/lahyc5ZG0zSpIWzqJGkrQ2bq2qR1TVw4DbgJeOnkwyrwY0VfWiqrp0jrvsRtOxTZKk32JRI0marzOBB7ajKGcm+SJwaZJlSY5Kcm6Sf0/yEmgW/Uzy3iTfTfJvwO9MP1GS05Ps2G4/KckFSb6T5KtJltMUTwe3o0SPTbJFkhPb1zg3yS7tY++V5JQklyT5IJA7+iGSfD7J+e1jDphx7l3t8a8m2aI99oAkX2ofc2aSbRbjzZQkzZ8tnSVJa60dkfkT4EvtoR2Ah1XVD9vC4Oaq2inJXYGvJzkF2B54MPBQ4HeBS4EPz3jeLYAPALu2z7VZVd2U5P3AL6rqHe39PgW8q6rOSnJf4MvAQ4DDgLOq6m+SPAXY/078OH/ZvsaGwLlJTqyqG4G7A+dV1cFJ3tw+9yuA44CXVtXlSf4X8D5gj3m8jZKkRWJRI0laGxsmubDdPhP4EM20sG9V1Q/b408Atpu+Xga4B/AHwK7A8VW1CrguyamzPP+jgDOmn6uqblpDjj8GHpr8z0DMJkk2al/jme1j/1+Sn96Jn+mgJM9ot7dus94IrAY+0x7/JPDP7WvsDJww8tp3vROvIUkaI4saSdLauLWqHjF6oP1wf8voIeDAqvryjPs9eRFzTAGPqqr/niXLnZZkN5oC6dFVtTLJ6cAGa7h7ta/7s5nvgSSpW15TI0labF8GXpZkPYAkD0pyd+AM4LntNTdbArvP8thvArsmuV/72M3a4yuAjUfudwpw4PROkuki4wxgn/bYnwD3vIOs9wB+2hY029CMFE2bAqZHm/ahmdb2c+CHSZ7TvkaS/OEdvIYkacwsaiRJi+2DNNfLXJDkYuAfaGYGfA64vD33ceDsmQ+sqp8AB9BM9foOt0//Ogl4xnSjAOAgYMe2EcGl3N6F7QiaougSmmloV99B1i8Bd0lyGfB2mqJq2i3AH7U/wx7A37TH/wzYv813CfC0O/GeSJLGKFXVdQZJkiRJmjdHaiRJkiT1mkWNJEmSpF6zqJEkSZLUaxY1kiRJknrNokaSJElSr1nUSJIkSeo1ixpJkiRJvWZRI0mSJKnX/j9t0Wugx0z0aQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x864 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}